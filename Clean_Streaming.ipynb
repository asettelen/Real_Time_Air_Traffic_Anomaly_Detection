{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "#import datefinder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import re   \n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "#from statsmodels.tsa.arima_model import ARIMA, ARIMAResults\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import datetime\n",
    "from fbprophet import Prophet\n",
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from pyspark.sql.types import *\n",
    "import math\n",
    "from threading import Thread\n",
    "import time\n",
    "from pyspark.sql.functions import collect_list, struct, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Spark RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tuple with len !=14\n",
    "def RemoveStrangeTupleLen(tup):\n",
    "    if len(tup)==14:\n",
    "        return tup\n",
    "    \n",
    "#CLEANING MAC_ADDRESS_SRC FIELD\n",
    "def RemoveQuoteSrc(tup):\n",
    "    if tup[0][0]=='\"':\n",
    "        tup[0]=tup[0][1:]\n",
    "    return tup\n",
    "\n",
    "def RemoveWeirdAddress(tup):\n",
    "    if len(tup[0])>17:\n",
    "        tup[0]=None\n",
    "    return tup\n",
    "\n",
    "#CLEANING CAT FIELD\n",
    "def CATToInt(tup): \n",
    "    tup[1] = int(tup[1])\n",
    "    return tup\n",
    "\n",
    "#CLEANING TID FIELD\n",
    "def replaceNullValue_TID(tup):\n",
    "    if tup[2] != '' and tup[2] != None and tup[2] != 'NaN': \n",
    "        return tup\n",
    "    else: \n",
    "        tup[2] = ''\n",
    "        return tup\n",
    "    \n",
    "#CLEANING TS FIELD\n",
    "def TSToFloat(tup): \n",
    "    tup[3] = float(round(float(tup[3])))\n",
    "    return tup\n",
    "\n",
    "#CLEANING DST FIELD\n",
    "#None\n",
    "\n",
    "#CLEANING SAC FIELD\n",
    "def replaceNullValue_SAC(tup):\n",
    "    if tup[5] != '' and tup[5] != None and tup[5] != 'NaN': \n",
    "        tup[5] = float(tup[5])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[5] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING SIC FIELD\n",
    "def replaceNullValue_SIC(tup):\n",
    "    if tup[6] != '' and tup[6] != None and tup[6] != 'NaN': \n",
    "        tup[6] = float(tup[6])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[6] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING ToD FIELD\n",
    "def replaceNullValue_ToD(tup):\n",
    "    if tup[7] != '' and tup[7] != None and tup[7] != 'NaN': \n",
    "        tup[7] = float(tup[7])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[7] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING TN FIELD\n",
    "def replaceNullValue_TN(tup):\n",
    "    if tup[8] != '' and tup[8] != None and tup[8] != 'NaN': \n",
    "        tup[8] = float(tup[8])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[8] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING THETA FIELD\n",
    "def replaceNullValue_THETA(tup):\n",
    "    if tup[9] != '' and tup[9] != None and tup[9] != 'NaN': \n",
    "        tup[9] = float(tup[9])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[9] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING RHO FIELD\n",
    "def replaceNullValue_RHO(tup):\n",
    "    if tup[10] != '' and tup[10] != None and tup[10] != 'NaN': \n",
    "        tup[10] = float(tup[10])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[10] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING FL FIELD \n",
    "def replaceNullValue_FL(tup):\n",
    "    if tup[11] != '' and tup[11] != None and tup[11] != 'NaN': \n",
    "        tup[11] = float(tup[11])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[11] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING CGS FIELD\n",
    "def replaceNullValue_CGS(tup):\n",
    "    if tup[12] != '' and tup[12] != None and tup[12] != 'NaN': \n",
    "        tup[12] = float(tup[12])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[12] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING CHdg FIELD\n",
    "    \n",
    "def replaceNullValue_CHdg(tup):\n",
    "    if tup[13] != '' and tup[13] != None and tup[13] != 'NaN': \n",
    "        tup[13] = float(tup[13])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[13] = None\n",
    "        return tup\n",
    "    \n",
    "def RemoveQuoteCHdg(tup):\n",
    "    if tup[13][-1]=='\"':\n",
    "        tup[13]=tup[13][:-1]\n",
    "    return tup\n",
    "\n",
    "\n",
    "def cleaning(tup):\n",
    "    tup = RemoveQuoteSrc(tup)\n",
    "    tup = RemoveWeirdAddress(tup)\n",
    "    tup = CATToInt(tup)\n",
    "    tup = replaceNullValue_TID(tup)\n",
    "    tup = TSToFloat(tup)\n",
    "    tup = replaceNullValue_SAC(tup)\n",
    "    tup = replaceNullValue_SIC(tup)\n",
    "    tup = replaceNullValue_ToD(tup)\n",
    "    tup = replaceNullValue_TN(tup) \n",
    "    tup = replaceNullValue_THETA(tup)\n",
    "    tup = replaceNullValue_RHO(tup)\n",
    "    tup = replaceNullValue_FL(tup)\n",
    "    tup = replaceNullValue_CGS(tup)\n",
    "    tup = replaceNullValue_CHdg(RemoveQuoteCHdg(tup))\n",
    "    return tup\n",
    "    \n",
    "def main_clean(rdd):\n",
    "    \n",
    "    #header = rdd.first()\n",
    "    #rdd = rdd.filter(lambda line: line != header)\n",
    "    #rdd = rdd.map(lambda tup: RemoveStrangeTupleLen(tup))\\\n",
    "    #         .filter(lambda tup: tup!=None)\n",
    "             \n",
    "    rdd = rdd.map(lambda tup: cleaning(tup))\n",
    "    return(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first need to import types (e.g. StructType, StructField, IntegerType, etc.)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def getlistavion(): \n",
    "    QUERY = 'SELECT DISTINCT(TID) FROM global_temp.traffic'\n",
    "    return spark.sql(QUERY).toPandas()\n",
    "\n",
    "def main_db(rdd): \n",
    "    global traffic_df_explicit, trafficSchema, spark  \n",
    "    \n",
    "    traffic_df_explicit_aux = spark.createDataFrame(rdd, trafficSchema)\n",
    "    traffic_df_explicit = traffic_df_explicit.unionAll(traffic_df_explicit_aux)       \n",
    "    \n",
    "    traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "    \n",
    "    traffic_df_explicit.cache()\n",
    "    \n",
    "    #Queries\n",
    "    #spark.sql(\"select TID, DST, COUNT(*) from global_temp.traffic WHERE TID != '' GROUP BY TID, DST ORDER BY COUNT(*) DESC\").show()\n",
    "    #spark.sql(\"select * from global_temp.traffic\").show()\n",
    "\n",
    "#main_db(rdd_traffic_clean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_m(row):\n",
    "    \"\"\"Transform data from pyspark.sql.Row to python dict to be used in rdd.\"\"\"\n",
    "    data = row['data']\n",
    "    tid = row['TID']\n",
    "    dst = row['DST']\n",
    "    \n",
    "    # Transform [pyspark.sql.Dataframe.Row] -> [dict]\n",
    "    data_dicts = []\n",
    "    for d in data:\n",
    "        data_dicts.append(d.asDict())\n",
    "\n",
    "    # Convert into pandas dataframe for fbprophet\n",
    "    data = pd.DataFrame(data_dicts)\n",
    "    data['ds'] = pd.to_datetime(data['ds'], unit='s')\n",
    "\n",
    "    return {\n",
    "        'tid' : tid,\n",
    "        'dst' : dst,\n",
    "        'data': data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data_m(d):\n",
    "    \"\"\"Split data into training and testing based on timestamp.\"\"\"\n",
    "    # Extract data from pd.Dataframe\n",
    "    data = d['data']\n",
    "\n",
    "    # Find max timestamp and extract timestamp for start of day\n",
    "    max_datetime = max(data['ds'])\n",
    "    #start_datetime = max_datetime.replace(hour=00, minute=00, second=00)\n",
    "\n",
    "    # Extract training data\n",
    "    train_data = data[data['ds'] <= max_datetime]\n",
    "\n",
    "    # Account for zeros in data while still applying uniform transform\n",
    "    #train_data['y'] = train_data['y'].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "    # Assign train/test split\n",
    "    #d['test_data'] = data.loc[(data['ds'] < start_datetime)\n",
    "    #                          & (data['ds'] <= max_datetime)]\n",
    "    d['train_data'] = train_data\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_m(d):\n",
    "    \"\"\"Create Prophet model using each input grid parameter set.\"\"\"\n",
    "    m = Prophet()\n",
    "    d['model'] = m\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_m(d):\n",
    "    \"\"\"Fit the model using the training data.\"\"\"\n",
    "    model = d['model']\n",
    "    train_data = d['train_data']\n",
    "    model.fit(train_data)\n",
    "    d['model'] = model\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecast_m(d):\n",
    "    \"\"\"Execute the forecast method on the model to make future predictions.\"\"\"\n",
    "    model = d['model']\n",
    "    future = model.make_future_dataframe(\n",
    "        periods=10, freq='4s')\n",
    "    \n",
    "    forecast = model.predict(future)\n",
    "    d['forecast'] = forecast\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_scope_m(d):\n",
    "    \"\"\"Return a tuple (app + , + metric_type, {}).\"\"\"\n",
    "    return (\n",
    "        d['tid'] + ',' + d['dst'],\n",
    "        {\n",
    "            'forecast': pd.concat([d['train_data']['y'],d['forecast']], axis=1),  \n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_predictions_m(d):\n",
    "    tid_dst, data = d\n",
    "    tid, dst = tid_dst.split(',')\n",
    "    return [\n",
    "        (\n",
    "            tid, \n",
    "            dst,\n",
    "            #p['ds'].strftime(\"%d-%b-%Y (%H:%M:%S)\"),\n",
    "            time.mktime(datetime.datetime.strptime(p['ds'].strftime(\"%d-%b-%Y (%H:%M:%S)\"), \"%d-%b-%Y (%H:%M:%S)\").timetuple()),\n",
    "            p['y'] if not(math.isnan(p['y'])) else None,\n",
    "            p['yhat'],\n",
    "            p['yhat_lower'],\n",
    "            p['yhat_upper'],\n",
    "        ) for i, p in data['forecast'].iterrows()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(var):\n",
    "    \n",
    "    global traffic_df_explicit, spark, schema_for_m\n",
    "    \n",
    "    traffic_for_m = traffic_df_explicit.select(\n",
    "                     traffic_df_explicit['TID'],\n",
    "                     traffic_df_explicit['DST'],                    \n",
    "                     traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                     traffic_df_explicit[var].alias('y'))\\\n",
    "                   .filter(\"TID like '%DSO05LM%' and DST like '%01:00:5e:50:01:42%'\")\\\n",
    "                   .groupBy('TID', 'DST')\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r: transform_data_m(r))\\\n",
    "                       .map(lambda d: partition_data_m(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model_m(d))\\\n",
    "                       .map(lambda d: train_model_m(d))\\\n",
    "                       .map(lambda d: make_forecast_m(d))\\\n",
    "                       .map(lambda d: reduce_data_scope_m(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions_m(d))\\\n",
    "        \n",
    "    traffic_for_m.cache()\n",
    "        \n",
    "    df_for_m = spark.createDataFrame(traffic_for_m, schema_for_m)\n",
    "            \n",
    "    #thread\n",
    "            \n",
    "    TH = Thread(target = forecast_from_spark, args=(df_for_m,var))\n",
    "    TH.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_from_spark(df, var):\n",
    "    \n",
    "    global nb_pred\n",
    "     #pas de show mais un filter sur les y == NAN pour n'envoyer que les forecast pour ces valeurs mais pas les anciennes\n",
    "    #df.show()\n",
    "    #df_for_m.filter(\" y == 'NaN'\").show() et et transformer y en cgs\n",
    "    #print(df.select('*').withColumnRenamed('y', var).show())\n",
    "    \n",
    "    if nb_pred == 0:\n",
    "        for line in df.filter(\" y IS NULL\").collect():\n",
    "            print(\"insert line\" + line)\n",
    "            #insert_table(var, connect(database_name='activus'), tid=line[0], dst=line[1], ds=line[2] , y='NULL', \n",
    "            #     yhat=line[4], yhat_lower=line[5], yhat_upper=line[6])\n",
    "            \n",
    "        nb_pred = 1 \n",
    "    \n",
    "    else: \n",
    "        i = 0\n",
    "        for line in df.filter(\" y IS NULL\").collect():\n",
    "            if (i < 5):\n",
    "                print(\"update line\" + line)\n",
    "                update_table(var, connect(database_name='activus'), tid=line[0], dst=line[1], ds=line[2] , y='NULL', \n",
    "                     yhat=line[4], yhat_lower=line[5], yhat_upper=line[6])\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else: \n",
    "                print(\"insert line\" + line)\n",
    "                insert_table(var, connect(database_name='activus'), tid=line[0], dst=line[1], ds=line[2] , y='NULL', \n",
    "                     yhat=line[4], yhat_lower=line[5], yhat_upper=line[6])\n",
    "        \n",
    "    \n",
    "    #disconnect('activus')\n",
    "\n",
    "    #pour chaque ligne du df \n",
    "        #insert_table(table_name, conn, tid, dst, ds, y, yhat, yhat_lower, yhat_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_from_spark(df, var):\n",
    "    \n",
    "    global nb_pred \n",
    "    #nb_pred = 1\n",
    "     #pas de show mais un filter sur les y == NAN pour n'envoyer que les forecast pour ces valeurs mais pas les anciennes\n",
    "    #df.show()\n",
    "    #df_for_m.filter(\" y == 'NaN'\").show() et et transformer y en cgs\n",
    "    #print(df.select('*').withColumnRenamed('y', var).show())\n",
    "    \n",
    "    #envoie de y et de la prédiction \n",
    "    if nb_pred == 0:\n",
    "        for line in df.collect():\n",
    "            print(\"insert line\", line)\n",
    "            #insert_table(var, connect(database_name='activus'), tid=line[0], dst=line[1], ds=line[2] , y='NULL', \n",
    "            #     yhat=line[4], yhat_lower=line[5], yhat_upper=line[6])\n",
    "            \n",
    "        nb_pred = 1 \n",
    "    \n",
    "    else: \n",
    "        i = 0\n",
    "        for line in df.collect():\n",
    "            \n",
    "            if line[3] != None:  \n",
    "                print(\"update line\", line)\n",
    "                #update_table(var, connect(database_name='activus'), tid=line[0], dst=line[1], ds=line[2] , y='NULL', \n",
    "                #     yhat=line[4], yhat_lower=line[5], yhat_upper=line[6])\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                if (i < 5):\n",
    "                    print(\"update line\", line)\n",
    "                    #update_table(var, connect(database_name='activus'), tid=line[0], dst=line[1], ds=line[2] , y='NULL', \n",
    "                    # yhat=line[4], yhat_lower=line[5], yhat_upper=line[6])\n",
    "\n",
    "                    i = i + 1\n",
    "\n",
    "                else: \n",
    "                    print(\"insert line\", line)\n",
    "                    #insert_table(var, connect(database_name='activus'), tid=line[0], dst=line[1], ds=line[2] , y='NULL', \n",
    "                    # yhat=line[4], yhat_lower=line[5], yhat_upper=line[6])\n",
    "        \n",
    "    \n",
    "    #disconnect('activus')\n",
    "\n",
    "    #pour chaque ligne du df \n",
    "        #insert_table(table_name, conn, tid, dst, ds, y, yhat, yhat_lower, yhat_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast_from_spark(df_for_m, \"CGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table(table_name, conn, tid, dst, ds, y, yhat, yhat_lower, yhat_upper):\n",
    "    TID_AUX = '\\'%' + str(tid).strip() + '%\\'' \n",
    "    DST_AUX = '\\'%' + str(dst).strip() + '%\\'' \n",
    "    \n",
    "    print(\"UPDATE \" + str(table_name) + \" SET yhat = \" + str(yhat) + \", yhat_lower = \" + str(yhat_lower) + \n",
    "      \", yhat_upper =  \" + str(yhat_lower) + \", DS = \" + str(float(ds))\n",
    "        + \" WHERE (DS = \" + str(float(ds)) + \" OR DS = \" + str(float(ds) - 1) + \n",
    "        \" OR DS = \" + str(float(ds) + 1) + \") AND y is NULL AND LTRIM(RTRIM(TID)) LIKE \" + \n",
    "        TID_AUX + \" AND LTRIM(RTRIM(DST)) LIKE \" + DST_AUX + \";\")\n",
    "    \n",
    "    '''\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"UPDATE \" + str(table_name) + \" SET yhat = \" + str(yhat) + \", yhat_lower = \" + str(yhat_lower) + \n",
    "      \", yhat_upper =  \" + str(yhat_lower) + \", DS = \" + str(float(ds))\n",
    "        + \" WHERE (DS = \" + str(float(ds)) + \" OR DS = \" + str(float(ds) - 1) + \n",
    "        \" OR DS = \" + str(float(ds) + 1) + \") AND y is NULL AND LTRIM(RTRIM(TID)) LIKE \" + \n",
    "        TID_AUX + \" AND LTRIM(RTRIM(DST)) LIKE \" + DST_AUX + \";\")   \n",
    "    conn.commit()\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_table(\"CGS\", '', tid=test[0][0], dst=test[0][1], ds=test[0][2] , y='NULL', \n",
    "                 yhat=test[0][4], yhat_lower=test[0][5], yhat_upper=test[0][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_table(\"CGS\", '', 'AAAAAA', '00:00:00:00:00', 1500000, 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_from_spark(df, var):\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql('select * from global_temp.traffic order by TS DESC').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select max(TS) from global_temp.traffic').collect()[0]['max(TS)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "float(spark.sql('select TID from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(spark.sql('select DST from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(spark.sql('select * from global_temp.traffic WHERE TS >= ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INSERT INTO INFO_TRAFFIC VALUES(\" + str(spark.sql('select max(TS) from global_temp.traffic').collect()[0]['max(TS)'] - 5) + \", \" \n",
    "      + str(spark.sql('select TID from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').distinct().count()) + \", \" +\n",
    "\n",
    "        str(spark.sql('select DST from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').distinct().count()) + \", \" +\n",
    "      \n",
    "        str(spark.sql('select * from global_temp.traffic WHERE TS >= ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').count()) + \");\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_table_info_traffic(table_name, conn, ds, nombre_avions, nombre_radars, nombre_paquets):\n",
    "    #cur = conn.cursor()\n",
    "    #print(\"INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s, %s);\"%(table_name, '\\'' + str(tid) + '\\'', '\\'' + str(dst) + '\\'', '\\'' + str(ds) + '\\'', float(y), float(yhat), float(yhat_lower), float(yhat_upper)))\n",
    "    \n",
    "    print(\"INSERT INTO \" + str(table_name) + \" VALUES(\" + str(ds - 5) + \", \" + str(nombre_avions) + \", \" +\n",
    "          str(nombre_radars) + \", \" + str(nombre_paquets) + \");\")\n",
    "    \n",
    "    #cur.execute(\"INSERT INTO \" + str(table_name) + \" VALUES(\" + str(ds - 5) + \", \" + str(nombre_avions) + \", \" +\n",
    "    #      str(nombre_radars) + \", \" + str(nombre_paquets) + \");\")\n",
    "\n",
    "    #conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_table_info_traffic(\"INFO_TRAFFIC\", \"\", spark.sql('select max(TS) from global_temp.traffic').collect()[0]['max(TS)'], \n",
    "                          spark.sql('select TID from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                          ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                          ORDER BY TS DESC').distinct().count(), \n",
    "                          spark.sql('select DST from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                          ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                          ORDER BY TS DESC').distinct().count(), \n",
    "                          spark.sql('select * from global_temp.traffic WHERE TS >= ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                          ORDER BY TS DESC').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_sup_traffic():\n",
    "    \n",
    "    '''\n",
    "   # -> nombre d'avions qui volent en temps réel (5 dernières secondes)\n",
    "    print('nombre avions courants : ' + str(spark.sql('select TID from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').distinct().count()))\n",
    "    \n",
    "    float(spark.sql('select TID from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').distinct().count())\n",
    "\n",
    "   # -> nombre de radars qui sont en train de les visualiser (5 dernières secondes)\n",
    "\n",
    "    print('nombre radars courants : ' + str(spark.sql('select DST from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').distinct().count()))\n",
    "    \n",
    "    float(spark.sql('select DST from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                        + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                    ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                    ORDER BY TS DESC').distinct().count()) \n",
    "\n",
    "\n",
    "  # -> nombre de paquets reçus en temps réel (5 dernières secondes)\n",
    "\n",
    "    print('nombre de paquets reçus par minute : ' + str(spark.sql('select * from global_temp.traffic WHERE TS >= ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').count()))\n",
    "    \n",
    "    float(spark.sql('select * from global_temp.traffic WHERE TS >= ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                ORDER BY TS DESC').count())\n",
    "    '''\n",
    "    \n",
    "    insert_table_info_traffic(\"INFO_TRAFFIC\", \"\", spark.sql('select max(TS) from global_temp.traffic').collect()[0]['max(TS)'], \n",
    "                          spark.sql('select TID from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                          ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                          ORDER BY TS DESC').distinct().count(), \n",
    "                          spark.sql('select DST from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS >= \\\n",
    "                          ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                          ORDER BY TS DESC').distinct().count(), \n",
    "                          spark.sql('select * from global_temp.traffic WHERE TS >= ((select max(TS) from global_temp.traffic) - 5) \\\n",
    "                          ORDER BY TS DESC').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_sup_traffic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976196.0  300.366211  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976199.0  303.662109  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976203.0  305.419922  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976207.0  301.245117  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976211.0  300.585938  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976215.0  303.222656  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976219.0  306.298828  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "29000\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976196E9', y=300.3662109375, yhat=300.3662109375, yhat_lower=300.3662109375, yhat_upper=300.3662109375)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976199E9', y=303.662109375, yhat=303.662109375, yhat_lower=303.662109375, yhat_upper=303.662109375)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976203E9', y=305.419921875, yhat=305.419921875, yhat_lower=305.419921875, yhat_upper=305.419921875)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976207E9', y=301.2451171875, yhat=301.2451171875, yhat_lower=301.2451171875, yhat_upper=301.2451171875)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976211E9', y=300.5859375, yhat=300.5859375, yhat_lower=300.5859375, yhat_upper=300.5859375)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976215E9', y=None, yhat=299.9267578125, yhat_lower=297.6580810546875, yhat_upper=302.1030578613281)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976219E9', y=None, yhat=299.267578125, yhat_lower=291.8170471191406, yhat_upper=308.1449279785156)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976223E9', y=None, yhat=298.6083984375, yhat_lower=283.3555908203125, yhat_upper=316.5164794921875)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976227E9', y=None, yhat=297.94921875, yhat_lower=272.958984375, yhat_upper=326.0692138671875)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976231E9', y=None, yhat=297.2900390625, yhat_lower=261.6896057128906, yhat_upper=335.53619384765625)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976235E9', y=None, yhat=296.630859375, yhat_lower=248.76658630371094, yhat_upper=348.04931640625)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976239E9', y=None, yhat=295.9716796875, yhat_lower=235.5483856201172, yhat_upper=362.3972473144531)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976243E9', y=None, yhat=295.3125, yhat_lower=220.40451049804688, yhat_upper=379.3244323730469)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976247E9', y=None, yhat=294.6533203125, yhat_lower=206.6512451171875, yhat_upper=393.71044921875)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976251E9', y=None, yhat=293.994140625, yhat_lower=190.6368865966797, yhat_upper=409.4286193847656)\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976223.0  307.397461  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976227.0  304.541016  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976231.0  301.904297  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976235.0  305.419922  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "47000\n",
      "48000\n",
      "49000\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976196E9', y=300.3662109375, yhat=300.3668212890625, yhat_lower=300.33123779296875, yhat_upper=300.4017333984375)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976199E9', y=303.662109375, yhat=303.6614990234375, yhat_lower=303.6235046386719, yhat_upper=303.6976623535156)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976203E9', y=305.419921875, yhat=305.41900634765625, yhat_lower=305.3843688964844, yhat_upper=305.4558410644531)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976207E9', y=301.2451171875, yhat=301.24603271484375, yhat_lower=301.2064208984375, yhat_upper=301.27923583984375)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976211E9', y=300.5859375, yhat=300.5859375, yhat_lower=300.5546569824219, yhat_upper=300.62353515625)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976215E9', y=303.22265625, yhat=303.22357177734375, yhat_lower=303.1876525878906, yhat_upper=303.25933837890625)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976219E9', y=306.298828125, yhat=306.29791259765625, yhat_lower=306.26239013671875, yhat_upper=306.3343811035156)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976223E9', y=307.3974609375, yhat=307.360595703125, yhat_lower=307.3238525390625, yhat_upper=307.3977966308594)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976227E9', y=304.541015625, yhat=304.6142578125, yhat_lower=304.57952880859375, yhat_upper=304.6502685546875)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976231E9', y=301.904296875, yhat=301.867919921875, yhat_lower=301.8317565917969, yhat_upper=301.90838623046875)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976235E9', y=None, yhat=299.1215515136719, yhat_lower=297.0904235839844, yhat_upper=300.8316345214844)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976239E9', y=None, yhat=296.3752136230469, yhat_lower=289.9486083984375, yhat_upper=302.5289001464844)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976243E9', y=None, yhat=293.62884521484375, yhat_lower=281.26702880859375, yhat_upper=306.3704833984375)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976247E9', y=None, yhat=290.88250732421875, yhat_lower=271.47314453125, yhat_upper=310.1333923339844)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976251E9', y=None, yhat=288.1361389160156, yhat_lower=259.1390075683594, yhat_upper=315.4799499511719)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976255E9', y=None, yhat=285.3898010253906, yhat_lower=246.67466735839844, yhat_upper=321.7156982421875)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976259E9', y=None, yhat=282.6434326171875, yhat_lower=233.21511840820312, yhat_upper=329.9827575683594)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976263E9', y=None, yhat=279.8970947265625, yhat_lower=219.1949005126953, yhat_upper=340.3193664550781)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976267E9', y=None, yhat=277.1507263183594, yhat_lower=203.62484741210938, yhat_upper=350.24127197265625)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976271E9', y=None, yhat=274.4043884277344, yhat_lower=188.213134765625, yhat_upper=361.02044677734375)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "51000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976239.0  306.079102  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976243.0  305.859375  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976247.0  310.253906  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976251.0  308.056641  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976255.0  308.276367  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976196E9', y=300.3662109375, yhat=302.0205078125, yhat_lower=299.57574462890625, yhat_upper=304.5803527832031)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976199E9', y=303.662109375, yhat=302.2789001464844, yhat_lower=299.7296142578125, yhat_upper=304.8559265136719)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976203E9', y=305.419921875, yhat=302.6234130859375, yhat_lower=300.07733154296875, yhat_upper=305.026123046875)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976207E9', y=301.2451171875, yhat=302.9679260253906, yhat_lower=300.70263671875, yhat_upper=305.2806091308594)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976211E9', y=300.5859375, yhat=303.31243896484375, yhat_lower=300.8341979980469, yhat_upper=305.9499206542969)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976215E9', y=303.22265625, yhat=303.6573181152344, yhat_lower=301.0553894042969, yhat_upper=306.18023681640625)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976219E9', y=306.298828125, yhat=304.0021667480469, yhat_lower=301.455078125, yhat_upper=306.5051574707031)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976223E9', y=307.3974609375, yhat=304.3438415527344, yhat_lower=301.8833923339844, yhat_upper=306.7550354003906)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976227E9', y=304.541015625, yhat=304.6804504394531, yhat_lower=302.0278015136719, yhat_upper=307.0177001953125)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976231E9', y=301.904296875, yhat=305.0170593261719, yhat_lower=302.6014709472656, yhat_upper=307.3588562011719)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976235E9', y=305.419921875, yhat=305.6907043457031, yhat_lower=303.46099853515625, yhat_upper=308.2034606933594)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976239E9', y=306.0791015625, yhat=306.3833312988281, yhat_lower=303.9773254394531, yhat_upper=308.8543701171875)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976243E9', y=305.859375, yhat=307.08074951171875, yhat_lower=304.67578125, yhat_upper=309.7252502441406)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976247E9', y=310.25390625, yhat=307.7781677246094, yhat_lower=305.2919616699219, yhat_upper=310.40631103515625)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976251E9', y=308.056640625, yhat=308.4755554199219, yhat_lower=306.0492858886719, yhat_upper=310.8848571777344)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976255E9', y=None, yhat=309.1729736328125, yhat_lower=306.841796875, yhat_upper=311.67852783203125)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976259E9', y=None, yhat=309.8703918457031, yhat_lower=307.5427551269531, yhat_upper=312.40118408203125)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976263E9', y=None, yhat=310.56781005859375, yhat_lower=308.06048583984375, yhat_upper=312.9806823730469)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976267E9', y=None, yhat=311.2652282714844, yhat_lower=308.90679931640625, yhat_upper=313.7933044433594)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976271E9', y=None, yhat=311.962646484375, yhat_lower=309.2183837890625, yhat_upper=314.48406982421875)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976275E9', y=None, yhat=312.6600646972656, yhat_lower=310.2264709472656, yhat_upper=315.25250244140625)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976279E9', y=None, yhat=313.35748291015625, yhat_lower=310.7306823730469, yhat_upper=315.7756042480469)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976283E9', y=None, yhat=314.0549011230469, yhat_lower=311.5364990234375, yhat_upper=316.5327453613281)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976287E9', y=None, yhat=314.7523193359375, yhat_lower=312.10198974609375, yhat_upper=317.4278259277344)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976291E9', y=None, yhat=315.4497375488281, yhat_lower=312.8661193847656, yhat_upper=318.1012268066406)\n",
      "74000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976259.0  313.330078  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976263.0  310.253906  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976267.0  312.890625  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976270.0  313.769531  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976274.0  309.814453  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "        tid                dst            ds         CGS  yhat yhat_lower  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  1556976278.0  312.890625  None       None   \n",
      "\n",
      "  yhat_upper  \n",
      "0       None  \n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976196E9', y=300.3662109375, yhat=301.8975830078125, yhat_lower=299.5956115722656, yhat_upper=304.24298095703125)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976199E9', y=303.662109375, yhat=302.1809997558594, yhat_lower=300.0052185058594, yhat_upper=304.4448547363281)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976203E9', y=305.419921875, yhat=302.55889892578125, yhat_lower=300.1314392089844, yhat_upper=304.7686462402344)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976207E9', y=301.2451171875, yhat=302.936767578125, yhat_lower=300.70819091796875, yhat_upper=305.13897705078125)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976211E9', y=300.5859375, yhat=303.3146667480469, yhat_lower=301.0762939453125, yhat_upper=305.761962890625)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976215E9', y=303.22265625, yhat=303.6970520019531, yhat_lower=301.4759521484375, yhat_upper=305.9248962402344)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976219E9', y=306.298828125, yhat=304.0794677734375, yhat_lower=301.67901611328125, yhat_upper=306.38250732421875)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976223E9', y=307.3974609375, yhat=304.46185302734375, yhat_lower=302.2377624511719, yhat_upper=306.86370849609375)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976227E9', y=304.541015625, yhat=304.84423828125, yhat_lower=302.5647277832031, yhat_upper=307.2806396484375)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976231E9', y=301.904296875, yhat=305.22662353515625, yhat_lower=302.8119812011719, yhat_upper=307.60028076171875)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976235E9', y=305.419921875, yhat=305.61993408203125, yhat_lower=303.3011474609375, yhat_upper=307.8933410644531)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976239E9', y=306.0791015625, yhat=306.01898193359375, yhat_lower=303.6329040527344, yhat_upper=308.367431640625)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976243E9', y=305.859375, yhat=306.9661560058594, yhat_lower=304.6966552734375, yhat_upper=309.2795104980469)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976247E9', y=310.25390625, yhat=307.9145812988281, yhat_lower=305.6123962402344, yhat_upper=310.23626708984375)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976251E9', y=308.056640625, yhat=308.86297607421875, yhat_lower=306.47650146484375, yhat_upper=311.22332763671875)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976255E9', y=308.2763671875, yhat=309.8114013671875, yhat_lower=307.5020751953125, yhat_upper=312.10565185546875)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976259E9', y=313.330078125, yhat=310.7597961425781, yhat_lower=308.22747802734375, yhat_upper=313.0880126953125)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976263E9', y=310.25390625, yhat=311.7082214355469, yhat_lower=309.31964111328125, yhat_upper=314.03460693359375)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976267E9', y=312.890625, yhat=312.6566162109375, yhat_lower=310.34527587890625, yhat_upper=314.98480224609375)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.55697627E9', y=313.76953125, yhat=313.3679504394531, yhat_lower=311.10595703125, yhat_upper=315.55792236328125)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976274E9', y=None, yhat=314.31634521484375, yhat_lower=311.9194641113281, yhat_upper=316.52081298828125)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976278E9', y=None, yhat=315.2647705078125, yhat_lower=312.9290771484375, yhat_upper=317.673583984375)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976282E9', y=None, yhat=316.2131652832031, yhat_lower=313.896728515625, yhat_upper=318.34234619140625)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976286E9', y=None, yhat=317.1615905761719, yhat_lower=314.6875, yhat_upper=319.4491271972656)\n",
      "update line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.55697629E9', y=None, yhat=318.1099853515625, yhat_lower=315.83636474609375, yhat_upper=320.3602600097656)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976294E9', y=None, yhat=319.0583801269531, yhat_lower=316.3763732910156, yhat_upper=321.63397216796875)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976298E9', y=None, yhat=320.0068054199219, yhat_lower=317.69970703125, yhat_upper=322.3343200683594)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976302E9', y=None, yhat=320.9552001953125, yhat_lower=318.5292053222656, yhat_upper=323.4191589355469)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.556976306E9', y=None, yhat=321.90362548828125, yhat_lower=319.3981018066406, yhat_upper=324.5113525390625)\n",
      "insert line Row(tid='DSO05LM ', dst='01:00:5e:50:01:42', ds='1.55697631E9', y=None, yhat=322.8520202636719, yhat_lower=320.4351806640625, yhat_upper=325.5378723144531)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98000\n",
      "99000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "traffic_df_explicit = spark.createDataFrame(spark.sparkContext.emptyRDD(),trafficSchema)\n",
    "traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "\n",
    "schema_for_m = StructType([\n",
    "        StructField(\"tid\", StringType(), True),\n",
    "        StructField(\"dst\", StringType(), True),\n",
    "        StructField(\"ds\", StringType(), True),\n",
    "        StructField(\"y\", FloatType(), True),\n",
    "        StructField(\"yhat\", FloatType(), True),\n",
    "        StructField(\"yhat_lower\", FloatType(), True),\n",
    "        StructField(\"yhat_upper\", FloatType(), True),\n",
    "    ])\n",
    "    \n",
    "list_aux = [] \n",
    "cmpt_tram = 0        \n",
    "\n",
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-12:00:00/2019-05-04-16:00:00', stream=True)\n",
    "#type(response)\n",
    "\n",
    "#Faire la même chose pour chdg et fl \n",
    "#create_table(table_name=\"cgs\", parameters=\"tid STRING, dst STRING, \\\n",
    "#             ds STRING, y FLOAT, yhat FLOAT, yhat_lower FLOAT, yhat_upper FLOAT\", database_name=\"activus\"):\n",
    "\n",
    "i = 0\n",
    "global nb_pred\n",
    "nb_pred = 0\n",
    "\n",
    "for data in response.iter_lines():\n",
    "    #print(data.decode(\"UTF-8\"))  \n",
    "    #print(str(data)[1:])\n",
    "    i = i + 1\n",
    "    #print(i)\n",
    "    #print([data.decode(\"UTF-8\").split(\",\")]) \n",
    "    #print(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    ligne = data.decode(\"UTF-8\").split(\",\")\n",
    "    list_aux.append(ligne)\n",
    "          \n",
    "    #prédire lorsque j'ai 5 nouveaux paquets pour un avion considere et un radar considere \n",
    "    #-> prediction \n",
    "    #ligne[2] : TID\n",
    "    #ligne[4] : DST\n",
    "    \n",
    "    #Pour l'avion et le radar considere\n",
    "    if('DSO05LM' in ligne[2] and '01:00:5e:50:01:42' in ligne[4]):\n",
    "        #compteur pour le nombre de tram   \n",
    "        #print(ligne)\n",
    "        \n",
    "        #print(compt_tram)\n",
    "        \n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        \n",
    "        #print(rdd_traffic_clean.collect())\n",
    "        main_db(rdd_traffic_clean) \n",
    "        \n",
    "        #faire un show pour un envoi en temps réel à la base de données sql\n",
    "        \n",
    "        cmpt_tram += 1 \n",
    "        list_aux = []\n",
    "        \n",
    "    #time series \n",
    "       \n",
    "        #envoie de la prédiction toutes les 5 trams\n",
    "        if(cmpt_tram==5):\n",
    "            \n",
    "            #faire la prédiction sur la variable de son choix \n",
    "            #pred(traffic_df_explicit, var='CGS')\n",
    "            #pred(traffic_df_explicit, var='CHdg')\n",
    "            #pred(traffic_df_explicit, var='FL')\n",
    "            \n",
    "            #pred(spark, traffic_df_explicit, schema_for_m)\n",
    "            \n",
    "            pred(var='CGS')\n",
    "            #pred(var='CHDG')\n",
    "            #pred(var='FL')\n",
    "         \n",
    "            '''\n",
    "            traffic_for_m = traffic_df_explicit.select(\n",
    "                     traffic_df_explicit['TID'],\n",
    "                     traffic_df_explicit['DST'],                    \n",
    "                     traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                     traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID like '%DSO05LM%' and DST like '%01:00:5e:50:01:42%'\")\\\n",
    "                   .groupBy('TID', 'DST')\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r: transform_data_m(r))\\\n",
    "                       .map(lambda d: partition_data_m(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model_m(d))\\\n",
    "                       .map(lambda d: train_model_m(d))\\\n",
    "                       .map(lambda d: make_forecast_m(d))\\\n",
    "                       .map(lambda d: reduce_data_scope_m(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions_m(d))\\\n",
    "        \n",
    "            traffic_for_m.cache()\n",
    "        \n",
    "            df_for_m = spark.createDataFrame(traffic_for_m, schema_for_m)\n",
    "            \n",
    "            #thread\n",
    "            \n",
    "            TH = Thread(target = forecast_from_spark, args=(df_for_m,))\n",
    "            TH.start()\n",
    "            \n",
    "            #TH = Thread(target = forecast_from_spark)\n",
    "            \n",
    "                     \n",
    "            #pas de show mais un filter sur les y == NAN pour n'envoyer que les forecast pour ces valeurs mais pas les \n",
    "            #anciennes\n",
    "            #df_for_m.show()\n",
    "            #envoie de y et de la prédiction \n",
    "            #pour chaque ligne du df \n",
    "                #insert_table(table_name, conn, tid, dst, ds, y, yhat, yhat_lower, yhat_upper)           \n",
    "            ###\n",
    "            \n",
    "            '''\n",
    "            #Réinitialisation du compteur\n",
    "            cmpt_tram=0\n",
    "            \n",
    "        #Envoie de y\n",
    "        #clean et envoi de la ligne à la volée\n",
    "        \n",
    "        #['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976195.58', '01:00:5e:50:01:42', '8', '80', '48194.625', '1253', '349.738769531', '10.65234375', '79.5', '300.366210938', '212.135009766']\n",
    "        \n",
    "        tid = ligne[2]\n",
    "        dst = ligne[4]\n",
    "        ds = str(float(round(float(ligne[3]))))   \n",
    "        src = ligne[0]\n",
    "        cat = int(ligne[1])\n",
    "        sac = float(ligne[5])\n",
    "        sic = float(ligne[6])\n",
    "        toD = ligne[7]\n",
    "        tn = float(ligne[8])\n",
    "        theta = float(ligne[9])\n",
    "        rho = float(ligne[10])\n",
    "        FL = float(ligne[11])\n",
    "        CGS = float(ligne[12])\n",
    "        CHdg = float(ligne[13])\n",
    "        yhat = None \n",
    "        yhat_lower = None\n",
    "        yhat_upper = None\n",
    "        \n",
    "        #d = {'tid': [tid], 'dst': [dst], 'ds': [ds], 'y': [y], 'yhat': [yhat], \n",
    "        #     'yhat_lower': [yhat_lower], 'yhat_upper': [yhat_upper]}\n",
    "        \n",
    "            #spark.createDataFrame(traffic_for_m, schema_for_m).show()\n",
    "        \n",
    "        #print(pd.DataFrame(data={'tid': [tid], 'dst': [dst], 'ds': [ds], 'FL': [FL], 'yhat': [yhat], \n",
    "        #     'yhat_lower': [yhat_lower], 'yhat_upper': [yhat_upper]}))\n",
    "        print(pd.DataFrame(data={'tid': [tid], 'dst': [dst], 'ds': [ds], 'CGS': [CGS], 'yhat': [yhat], \n",
    "             'yhat_lower': [yhat_lower], 'yhat_upper': [yhat_upper]}))\n",
    "        \n",
    "        #print(pd.DataFrame(data={'tid': [tid], 'dst': [dst], 'ds': [ds], 'CHdg': [CHdg], 'yhat': [yhat], \n",
    "        #     'yhat_lower': [yhat_lower], 'yhat_upper': [yhat_upper]}))\n",
    "        \n",
    "        #print(pd.DataFrame(data={'tid': [tid], 'dst': [dst], 'ds': [ds], 'src':[src], 'cat':[cat], 'sac':[sac], \n",
    "        #                         'sic':[sic], 'tod':[toD], 'tn':[tn], 'theta':[theta], 'rho':[rho], \n",
    "        #                         'fl':[FL], 'cgs':[CGS], 'chdg':[CHdg]}))\n",
    "        \n",
    "        #insert_table('CHDG', connect(database_name='activus'), tid=tid, dst=dst, ds=ds , y=CHdg, yhat='NULL', yhat_lower='NULL', yhat_upper='NULL')\n",
    "        \n",
    "        #insert_table('CGS', connect(database_name='activus'), tid=tid, dst=dst, ds=ds , y=CGS, yhat='NULL', yhat_lower='NULL', yhat_upper='NULL')\n",
    "        \n",
    "        #insert_table('FL', connect(database_name='activus'), tid=tid, dst=dst, ds=ds , y=FL, yhat='NULL', yhat_lower='NULL', yhat_upper='NULL')\n",
    "        \n",
    "        #insert_table_fields('FIELDS', connect(database_name='activus'), tid=tid, dst=dst, ds=ds, src=src, cat=cat, sac=sac, \n",
    "        #                    sic=sic, tod=toD, tn=tn, theta=theta, rho=rho, fl=FL, cgs=CGS, chdg=chdg)\n",
    "        \n",
    "    '''            \n",
    "    def insert_table_fields(table_name, conn, tid, dst, ds, src, cat, sac, sic, tod, tn, theta, rho, fl, cgs, chdg):\n",
    "        cur = conn.cursor()\n",
    "        #print(\"INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s, %s);\"%(table_name, '\\'' + str(tid) + '\\'', '\\'' + str(dst) + '\\'', '\\'' + str(ds) + '\\'', float(y), float(yhat), float(yhat_lower), float(yhat_upper)))\n",
    "        cur.execute(\"INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\"%(table_name, '\\'' + \n",
    "        str(tid) + '\\'', '\\'' + str(dst) + '\\'', '\\'' + str(ds) + '\\'', '\\'' + str(src) + '\\'', cat, sac, sic, \n",
    "        '\\'' + str(tod) + '\\'', tn, theta, rho, fl, cgs, chdg))\n",
    "        #cur.execute(\"INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s, %s);\"%(table_name, tid, dst, ds, float(y), float(yhat), float(yhat_lower), float(yhat_upper)))\n",
    "        conn.commit()\n",
    "    '''\n",
    "        \n",
    "        #disconnect('activus')\n",
    "        \n",
    "            #insert_table(table_name, conn, tid, dst, ds, y, yhat, yhat_lower, yhat_upper)\n",
    "            \n",
    "    #nombre d'avion en fonction d'une période de temps \n",
    "    \n",
    "    #if(not(i%5000)): \n",
    "        #info courante \n",
    "    #    TH_current = Thread(target =  info_sup_traffic)\n",
    "    #    TH_current.start()\n",
    "    \n",
    "    if(not(i%1000)):\n",
    "        print(i)\n",
    "        \n",
    "    if (i==100000): break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "traffic_df_explicit = spark.createDataFrame(spark.sparkContext.emptyRDD(),trafficSchema)\n",
    "traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "\n",
    "schema_for_m = StructType([\n",
    "        StructField(\"tid\", StringType(), True),\n",
    "        StructField(\"dst\", StringType(), True),\n",
    "        StructField(\"ds\", StringType(), True),\n",
    "        StructField(\"y\", FloatType(), True),\n",
    "        StructField(\"yhat\", FloatType(), True),\n",
    "        StructField(\"yhat_lower\", FloatType(), True),\n",
    "        StructField(\"yhat_upper\", FloatType(), True),\n",
    "    ])\n",
    "    \n",
    "list_aux = [] \n",
    "cmpt_tram = 0        \n",
    "\n",
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-12:00:00/2019-05-04-16:00:00', stream=True)\n",
    "i = 0\n",
    "for data in response.iter_lines():\n",
    "    i = i + 1  \n",
    "    ligne = data.decode(\"UTF-8\").split(\",\")\n",
    "    list_aux.append(ligne)\n",
    "    \n",
    "    #Pour l'avion et le radar considere\n",
    "    if('DSO05LM' in ligne[2] and '01:00:5e:50:01:42' in ligne[4]):\n",
    "        \n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        main_db(rdd_traffic_clean) \n",
    "        \n",
    "        cmpt_tram += 1 \n",
    "        list_aux = []\n",
    "       \n",
    "        #envoie de la prédiction toutes les 5 trams\n",
    "        if(cmpt_tram==5):\n",
    "            \n",
    "            #faire la prédiction sur la variable de son choix \n",
    "            pred(var='CGS')\n",
    "            pred(var='CHDG')\n",
    "            pred(var='FL')\n",
    "         \n",
    "            #Réinitialisation du compteur\n",
    "            cmpt_tram=0\n",
    "            \n",
    "        #Envoie de y\n",
    "        #clean et envoi de la ligne à la volée\n",
    "        \n",
    "        tid, dst, ds, src, cat = ligne[2], ligne[4], str(float(round(float(ligne[3])))), ligne[0], int(ligne[1])   \n",
    "        sac, sic, toD, tn, theta = float(ligne[5]), float(ligne[6]), ligne[7], float(ligne[8]), float(ligne[9])\n",
    "        rho, FL, CGS, CHdg = float(ligne[10]), float(ligne[11]), float(ligne[12]), float(ligne[13])\n",
    "        yhat, yhat_lower, yhat_upper = None, None, None \n",
    "        \n",
    "        insert_table('CHDG', connect(database_name='activus'), tid=tid, dst=dst, ds=ds , y=CHdg, yhat='NULL', yhat_lower='NULL', yhat_upper='NULL')\n",
    "        \n",
    "        insert_table('CGS', connect(database_name='activus'), tid=tid, dst=dst, ds=ds , y=CGS, yhat='NULL', yhat_lower='NULL', yhat_upper='NULL')\n",
    "        \n",
    "        insert_table('FL', connect(database_name='activus'), tid=tid, dst=dst, ds=ds , y=FL, yhat='NULL', yhat_lower='NULL', yhat_upper='NULL')\n",
    "        \n",
    "        insert_table_fields('FIELDS', connect(database_name='activus'), tid=tid, dst=dst, ds=ds, src=src, cat=cat, sac=sac, \n",
    "                            sic=sic, tod=toD, tn=tn, theta=theta, rho=rho, fl=FL, cgs=CGS, chdg=chdg)\n",
    "        \n",
    "        disconnect('activus')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> nombre d'avions qui volent en temps réel\n",
    "\n",
    "traffic_df_explicit.select(\n",
    "                     traffic_df_explicit['TID'],\n",
    "                     traffic_df_explicit['DST'],                    \n",
    "                     traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                     traffic_df_explicit[var].alias('y'))\\\n",
    "                   .filter(\"TID like '%DSO05LM%' and DST like '%01:00:5e:50:01:42%'\")\\\n",
    "                   .groupBy('TID', 'DST')\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r: transform_data_m(r))\\\n",
    "                       .map(lambda d: partition_data_m(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model_m(d))\\\n",
    "                       .map(lambda d: train_model_m(d))\\\n",
    "                       .map(lambda d: make_forecast_m(d))\\\n",
    "                       .map(lambda d: reduce_data_scope_m(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions_m(d))\\\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> nombre d'avions qui volent en temps réel (sur une plage de temps de 10 secondes) \n",
    "#prenant en compte le TS du packet courant moins 10\n",
    "\n",
    "TS_AUX = 1.556976278E9\n",
    "float(TS_AUX - 10)\n",
    "\n",
    "#print('select distinct(TS) from global_temp.traffic WHERE TS > ' + str(TS_AUX - 10) + ' ORDER BY TS DESC')\n",
    "\n",
    "#spark.sql('select distinct(TS) from global_temp.traffic WHERE TS >' + '\\' 1556976268.0 ORDER BY TS DESC\").show()\n",
    "\n",
    "spark.sql('select * from global_temp.traffic WHERE TS > ' \n",
    "          + str(TS_AUX - 60) + ' ORDER BY TS DESC').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def info_sup_traffic(ts_courant):\n",
    "   # -> nombre d'avions qui volent en temps réel\n",
    "\n",
    "  print('nombre avions courants : ' + str(spark.sql('select TID from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' \n",
    "                                    + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS > ' \n",
    "          + str(float(ts_courant) - 10) + ' ORDER BY TS DESC').distinct().count()))\n",
    "\n",
    "   # -> nombre de radars qui sont en train de les visualiser \n",
    "\n",
    "  print('nombre radars courants : ' + str(spark.sql('select DST from global_temp.traffic WHERE LTRIM(RTRIM(TID)) NOT LIKE' + '\\'' + '\\' ' + 'AND TS IS NOT NULL AND TS > ' \n",
    "          + str(TS_AUX - 10) + ' ORDER BY TS DESC').distinct().count()))\n",
    "    \n",
    "   # -> nombre de paquets reçus par minute \n",
    "\n",
    "  print('nombre de paquets reçus par minute : ' + str(spark.sql('select * from global_temp.traffic WHERE TS > ' \n",
    "          + str(TS_AUX - 60) + ' ORDER BY TS DESC').count()))\n",
    "          \n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_sup_traffic('1556976278.58')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "traffic_df_explicit = spark.createDataFrame(spark.sparkContext.emptyRDD(),trafficSchema)\n",
    "traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "\n",
    "schema_for_m = StructType([\n",
    "        StructField(\"tid\", StringType(), True),\n",
    "        StructField(\"dst\", StringType(), True),\n",
    "        StructField(\"ds\", StringType(), True),\n",
    "        StructField(\"y\", FloatType(), True),\n",
    "        StructField(\"yhat\", FloatType(), True),\n",
    "        StructField(\"yhat_lower\", FloatType(), True),\n",
    "        StructField(\"yhat_upper\", FloatType(), True),\n",
    "    ])\n",
    "    \n",
    "list_aux = [] \n",
    "cmpt_tram = 0        \n",
    "\n",
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-12:00:00/2019-05-04-16:00:00', stream=True)\n",
    "#type(response)\n",
    "\n",
    "#Faire la même chose pour chdg et fl \n",
    "#create_table(table_name=\"cgs\", parameters=\"tid STRING, dst STRING, \\\n",
    "#             ds STRING, y FLOAT, yhat FLOAT, yhat_lower FLOAT, yhat_upper FLOAT\", database_name=\"activus\"):\n",
    "\n",
    "i = 0\n",
    "\n",
    "for data in response.iter_lines():\n",
    "    #print(data.decode(\"UTF-8\"))  \n",
    "    #print(str(data)[1:])\n",
    "    i = i + 1\n",
    "    #print(i)\n",
    "    #print([data.decode(\"UTF-8\").split(\",\")]) \n",
    "    #print(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    ligne = data.decode(\"UTF-8\").split(\",\")\n",
    "    list_aux.append(ligne)\n",
    "    \n",
    "      \n",
    "    #prédire lorsque j'ai 5 nouveaux paquets pour un avion considere et un radar considere \n",
    "    #-> prediction \n",
    "    #ligne[2] : TID\n",
    "    #ligne[4] : DST\n",
    "    \n",
    "    #Pour l'avion et le radar considere\n",
    "    if('DSO05LM' in ligne[2] and '01:00:5e:50:01:42' in ligne[4]):\n",
    "        #compteur pour le nombre de tram   \n",
    "        #print(ligne)\n",
    "        \n",
    "        #print(compt_tram)\n",
    "        \n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        \n",
    "        #print(rdd_traffic_clean.collect())\n",
    "        main_db(rdd_traffic_clean) \n",
    "        \n",
    "        #faire un show pour un envoi en temps réel à la base de données sql\n",
    "        \n",
    "        cmpt_tram += 1 \n",
    "        list_aux = []     \n",
    "        \n",
    "    #time series \n",
    "       \n",
    "        #envoie de la prédiction toutes les 5 trams\n",
    "        if(cmpt_tram==5):\n",
    "            \n",
    "            #faire la prédiction sur la variable de son choix \n",
    "            #pred(traffic_df_explicit, var='CGS')\n",
    "            #pred(traffic_df_explicit, var='CHdg')\n",
    "            #pred(traffic_df_explicit, var='FL')\n",
    "            \n",
    "            #pred(spark, traffic_df_explicit, schema_for_m)\n",
    "            \n",
    "            #pred(var='CGS')\n",
    "            #pred(var='CHdg')\n",
    "            #pred(var='FL')\n",
    "         \n",
    "            traffic_for_m = traffic_df_explicit.select(\n",
    "                     traffic_df_explicit['TID'],\n",
    "                     traffic_df_explicit['DST'],                    \n",
    "                     traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                     traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID like '%DSO05LM%' and DST like '%01:00:5e:50:01:42%'\")\\\n",
    "                   .groupBy('TID', 'DST')\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r: transform_data_m(r))\\\n",
    "                       .map(lambda d: partition_data_m(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model_m(d))\\\n",
    "                       .map(lambda d: train_model_m(d))\\\n",
    "                       .map(lambda d: make_forecast_m(d))\\\n",
    "                       .map(lambda d: reduce_data_scope_m(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions_m(d))\\\n",
    "        \n",
    "            traffic_for_m.cache()\n",
    "        \n",
    "            df_for_m = spark.createDataFrame(traffic_for_m, schema_for_m)\n",
    "            \n",
    "            #thread\n",
    "            \n",
    "            #TH = Thread(target = forecast_from_spark, args=(df_for_m,))\n",
    "            #TH.start()\n",
    "            \n",
    "            #TH = Thread(target = forecast_from_spark)\n",
    "            \n",
    "                     \n",
    "            #pas de show mais un filter sur les y == NAN pour n'envoyer que les forecast pour ces valeurs mais pas les \n",
    "            #anciennes\n",
    "            df_for_m.show()\n",
    "            #envoie de y et de la prédiction \n",
    "            #pour chaque ligne du df \n",
    "                #insert_table(table_name, conn, tid, dst, ds, y, yhat, yhat_lower, yhat_upper)           \n",
    "            ###\n",
    "            \n",
    "            #Réinitialisation du compteur\n",
    "            cmpt_tram=0\n",
    "            \n",
    "        #Envoie de y\n",
    "        #clean et envoi de la ligne à la volée\n",
    "        tid = ligne[2]\n",
    "        dst = ligne[4]\n",
    "        ds = float(round(float(ligne[3])))\n",
    "        FL = float(ligne[11])\n",
    "        CGS = float(ligne[12])\n",
    "        CHdg = float(ligne[13])\n",
    "        yhat = None \n",
    "        yhat_lower = None\n",
    "        yhat_upper = None\n",
    "        \n",
    "        #d = {'tid': [tid], 'dst': [dst], 'ds': [ds], 'y': [y], 'yhat': [yhat], \n",
    "        #     'yhat_lower': [yhat_lower], 'yhat_upper': [yhat_upper]}\n",
    "        \n",
    "            #spark.createDataFrame(traffic_for_m, schema_for_m).show()\n",
    "        \n",
    "        print(pd.DataFrame(data={'tid': [tid], 'dst': [dst], 'ds': [ds], 'FL': [FL], 'yhat': [yhat], \n",
    "             'yhat_lower': [yhat_lower], 'yhat_upper': [yhat_upper]}))\n",
    "        print(pd.DataFrame(data={'tid': [tid], 'dst': [dst], 'ds': [ds], 'CGS': [CGS], 'yhat': [yhat], \n",
    "             'yhat_lower': [yhat_lower], 'yhat_upper': [yhat_upper]}))\n",
    "        print(pd.DataFrame(data={'tid': [tid], 'dst': [dst], 'ds': [ds], 'CHdg': [CHdg], 'yhat': [yhat], \n",
    "             'yhat_lower': [yhat_lower], 'yhat_upper': [yhat_upper]}))\n",
    "        \n",
    "            #insert_table(table_name, conn, tid, dst, ds, y, yhat, yhat_lower, yhat_upper)\n",
    "            \n",
    "    \n",
    "    if(not(i%1000)):\n",
    "        print(i)\n",
    "        \n",
    "    if (i==100000): break \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import requests\n",
    "import mysql.connector as mconn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_database(database_name=\"activus\"):\n",
    "    '''\n",
    "    create a database using: CREATE DATABASE database_name;\n",
    "    select a database using: USE database_name '''\n",
    "    conn = mconn.connect(host=\"192.168.37.86\", port=3306, user=\"root\", password=\"secret\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"CREATE DATABASE %s;\"%(database_name))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def show_available_databases():\n",
    "    conn = mconn.connect(host=\"192.168.37.86\", port=3306, user=\"azerty\", password=\"azerty\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SHOW DATABASES;\")\n",
    "    databases = cur.fetchall()\n",
    "    for database in databases:\n",
    "        print(database)\n",
    "    conn.close()\n",
    "\n",
    "def create_table(table_name=\"tableX\", parameters=\"x INT, y FLOAT\", database_name=\"activus\"):\n",
    "    conn = mconn.connect(host=\"localhost\", port=3306, user=\"root\", password=\"secret\", database=database_name)\n",
    "    cur = conn.cursor()\n",
    "    n = cur.execute(\"CREATE TABLE IF NOT EXISTS %s (%s);\"%(table_name, parameters))\n",
    "    print(n)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def show_available_tables(database_name='activus'):\n",
    "    conn = mconn.connect(host=\"192.168.37.86\", port=3306, user=\"azerty\", password=\"azerty\", database=database_name)\n",
    "    cur = conn.cursor()\n",
    "    tables = cur.execute(\"SHOW TABLES FROM %s;\" % (database_name))\n",
    "    #tables = cur.execute(\"SHOW TABLES\")\n",
    "    print(tables)\n",
    "    if not tables: return  \n",
    "    for table in tables:\n",
    "        print(table)\n",
    "    conn.close()\n",
    "\n",
    "def drop_table(table_name=\"tabl1\", database_name=\"test\"):\n",
    "    conn = mconn.connect(host=\"localhost\", port=3306, user=\"root\", password=\"secret\", database=database_name)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"DROP TABLE %s;\"%(table_name))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def draft_populate(table_name=\"tableX\", database_name=\"test\"):\n",
    "    conn = mconn.connect(host=\"localhost\", port=3306, user=\"root\", password=\"secret\", database=database_name)\n",
    "    cur = conn.cursor()\n",
    "    datas = requests.get(url=\"http://192.168.37.142:50005/stream/2019-05-04-00:00:00/2019-05-04-06:00:00\", stream=True)\n",
    "    tsOld=0\n",
    "    n=0\n",
    "    # ts = 2020-01-02 23!12:32\n",
    "    for line in datas.iter_lines():\n",
    "        src, cat, tid, ts, dst, sac, sic, tod, tn, theta, rho, fl, cgs, chdg = line.split(\",\")\n",
    "        ts = Integer.par(ts)\n",
    "        #t = datetime.strptime(ts,\"%Y-%m-%d %H:%M:%S\")\n",
    "        if ts-tsOld>5:\n",
    "            tsOld=ts\n",
    "            n=0\n",
    "        n+=1\n",
    "        cur.execute(\"INSERT INTO %s VALUES (%s, %s, %s, %s);\"%(table_name,(t-datetime(1970,1,1)).total_seconds(), float(y), float(yhat_lower), float(yhat_upper)))\n",
    "        conn.commit()\n",
    "        sleep(2)\n",
    "    conn.close()\n",
    "    \n",
    "def connect(database_name):\n",
    "    conn = mconn.connect(host=\"192.168.37.86\", port=3306, user=\"azerty\", password=\"azerty\", database=database_name)\n",
    "    return conn\n",
    "\n",
    "parameters=\"tid STRING, dst STRING, \\\n",
    "#             ds STRING, y FLOAT, yhat FLOAT, yhat_lower FLOAT, yhat_upper FLOAT\"\n",
    "    \n",
    "def query_from_table(table_name=\"tabl1\", database_name=\"test\"):\n",
    "    conn = mconn.connect(host=\"192.168.37.86\", port=3306, user=\"azerty\", password=\"azerty\", database=database_name)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT * FROM %s;\"%(table_name))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        \n",
    "def insert_table(table_name, conn, tid, dst, ds, y, yhat, yhat_lower, yhat_upper):\n",
    "    cur = conn.cursor()\n",
    "    #print(\"INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s, %s);\"%(table_name, '\\'' + str(tid) + '\\'', '\\'' + str(dst) + '\\'', '\\'' + str(ds) + '\\'', float(y), float(yhat), float(yhat_lower), float(yhat_upper)))\n",
    "    cur.execute(\"INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s, %s);\"%(table_name, '\\'' + str(tid) + '\\'', '\\'' + str(dst) + '\\'', '\\'' + str(ds) + '\\'', y, yhat, yhat_lower, yhat_upper))\n",
    "    #cur.execute(\"INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s, %s);\"%(table_name, tid, dst, ds, float(y), float(yhat), float(yhat_lower), float(yhat_upper)))\n",
    "    conn.commit()\n",
    "    \n",
    "def update_table(table_name, conn, tid, dst, ds, y, yhat, yhat_lower, yhat_upper):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"UPDATE \" + str(table_name) + \" SET yhat = \" + str(yhat) + \", yhat_lower = \" + str(yhat_lower) + \n",
    "      \", yhat_upper =  \" + str(yhat_lower)\n",
    "        + \" WHERE (DS = \" + str(ds) + \" OR DS = \" + str(ds - 1) + \n",
    "        \" OR DS = \" + str(ds + 1) + \") AND y is NULL AND LTRIM(RTRIM(TID)) LIKE \" + \n",
    "        str(tid) + \"AND LTRIM(RTRIM(DST)) LIKE \" + str(dst) + \";\")   \n",
    "    conn.commit()\n",
    "    \n",
    "def disconnect(database_name):\n",
    "    mconn.connect(host=\"192.168.37.86\", port=3306, user=\"azerty\", password=\"azerty\", database=database_name).close()\n",
    "\n",
    "#create_database()\n",
    "#show_available_databases()\n",
    "# create_table(table_name=\"planes_trend\", parameters=\"ts TIMESTAMP, number_of_planes INT\")\n",
    "#show_available_tables()\n",
    "#show_available_tables()\n",
    "#draft_populate()\n",
    "\n",
    "\n",
    "\n",
    "query_from_table(table_name = 'CHDG', database_name='activus')\n",
    "# drop_table()\n",
    "\n",
    "#show_available_databases()\n",
    "\n",
    "#insert_table('CHDG', connect(database_name='activus'), tid='yolo', dst='yolo', ds='yolo', y=0, yhat=0, yhat_lower=0, yhat_upper=0)\n",
    "\n",
    "#insert_table('CHDG', connect(database_name='activus'), tid='yolo', dst='yolo', ds='yolo', y='NULL', yhat='NULL', yhat_lower='NULL', yhat_upper='NULL')\n",
    "#disconnect('activus')\n",
    "#connect(database_name)\n",
    "#conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATE CGS\n",
    "#SET yhat = 0, yhat_lower = 0, yhat_upper = 0\n",
    "#WHERE (DS = 1556976215 OR DS = 1556976215 - 1 OR DS = 1556976215 + 1)  AND y IS NULL \n",
    "\n",
    "DS_AUX = 1556976215\n",
    "table_name = \"CGS\"\n",
    "yhat = 0.0\n",
    "yhat_lower = 0.0\n",
    "yhat_upper = 0.0 \n",
    "TID_AUX = '\\'%' + 'DSO05LM '.lstrip().rstrip() + '%\\'' \n",
    "DST_AUX = '\\'%' + '01:00:5e:50:01:42'.lstrip().rstrip() + '%\\'' \n",
    "    \n",
    "print(\"UPDATE \" + str(table_name) + \" SET yhat = \" + str(yhat) + \", yhat_lower = \" + str(yhat_lower) + \n",
    "      \", yhat_upper =  \" + str(yhat_lower)\n",
    "        + \" WHERE (DS = \" + str(DS_AUX) + \" OR DS = \" + str(DS_AUX - 1) + \n",
    "        \" OR DS = \" + str(DS_AUX + 1) + \") AND y is NULL AND LTRIM(RTRIM(TID)) LIKE \" + \n",
    "        str(TID_AUX) + \"AND LTRIM(RTRIM(DST)) LIKE \" + str(DST_AUX) + \";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
