{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "#import datefinder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import re   \n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "#from statsmodels.tsa.arima_model import ARIMA, ARIMAResults\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import datetime\n",
    "from fbprophet import Prophet\n",
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Spark RDD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tuple with len !=14\n",
    "def RemoveStrangeTupleLen(tup):\n",
    "    if len(tup)==14:\n",
    "        return tup\n",
    "    \n",
    "#CLEANING MAC_ADDRESS_SRC FIELD\n",
    "def RemoveQuoteSrc(tup):\n",
    "    if tup[0][0]=='\"':\n",
    "        tup[0]=tup[0][1:]\n",
    "    return tup\n",
    "\n",
    "def RemoveWeirdAddress(tup):\n",
    "    if len(tup[0])>17:\n",
    "        tup[0]=None\n",
    "    return tup\n",
    "\n",
    "#CLEANING CAT FIELD\n",
    "def CATToInt(tup): \n",
    "    tup[1] = int(tup[1])\n",
    "    return tup\n",
    "\n",
    "#CLEANING TID FIELD\n",
    "def replaceNullValue_TID(tup):\n",
    "    if tup[2] != '' and tup[2] != None and tup[2] != 'NaN': \n",
    "        return tup\n",
    "    else: \n",
    "        tup[2] = ''\n",
    "        return tup\n",
    "    \n",
    "#CLEANING TS FIELD\n",
    "def TSToFloat(tup): \n",
    "    tup[3] = float(round(float(tup[3])))\n",
    "    return tup\n",
    "\n",
    "#CLEANING DST FIELD\n",
    "#None\n",
    "\n",
    "#CLEANING SAC FIELD\n",
    "def replaceNullValue_SAC(tup):\n",
    "    if tup[5] != '' and tup[5] != None and tup[5] != 'NaN': \n",
    "        tup[5] = float(tup[5])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[5] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING SIC FIELD\n",
    "def replaceNullValue_SIC(tup):\n",
    "    if tup[6] != '' and tup[6] != None and tup[6] != 'NaN': \n",
    "        tup[6] = float(tup[6])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[6] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING ToD FIELD\n",
    "def replaceNullValue_ToD(tup):\n",
    "    if tup[7] != '' and tup[7] != None and tup[7] != 'NaN': \n",
    "        tup[7] = float(tup[7])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[7] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING TN FIELD\n",
    "def replaceNullValue_TN(tup):\n",
    "    if tup[8] != '' and tup[8] != None and tup[8] != 'NaN': \n",
    "        tup[8] = float(tup[8])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[8] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING THETA FIELD\n",
    "def replaceNullValue_THETA(tup):\n",
    "    if tup[9] != '' and tup[9] != None and tup[9] != 'NaN': \n",
    "        tup[9] = float(tup[9])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[9] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING RHO FIELD\n",
    "def replaceNullValue_RHO(tup):\n",
    "    if tup[10] != '' and tup[10] != None and tup[10] != 'NaN': \n",
    "        tup[10] = float(tup[10])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[10] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING FL FIELD \n",
    "def replaceNullValue_FL(tup):\n",
    "    if tup[11] != '' and tup[11] != None and tup[11] != 'NaN': \n",
    "        tup[11] = float(tup[11])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[11] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING CGS FIELD\n",
    "def replaceNullValue_CGS(tup):\n",
    "    if tup[12] != '' and tup[12] != None and tup[12] != 'NaN': \n",
    "        tup[12] = float(tup[12])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[12] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING CHdg FIELD\n",
    "    \n",
    "def replaceNullValue_CHdg(tup):\n",
    "    if tup[13] != '' and tup[13] != None and tup[13] != 'NaN': \n",
    "        tup[13] = float(tup[13])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[13] = None\n",
    "        return tup\n",
    "    \n",
    "def RemoveQuoteCHdg(tup):\n",
    "    if tup[13][-1]=='\"':\n",
    "        tup[13]=tup[13][:-1]\n",
    "    return tup\n",
    "\n",
    "\n",
    "def cleaning(tup):\n",
    "    tup = RemoveQuoteSrc(tup)\n",
    "    tup = RemoveWeirdAddress(tup)\n",
    "    tup = CATToInt(tup)\n",
    "    tup = replaceNullValue_TID(tup)\n",
    "    tup = TSToFloat(tup)\n",
    "    tup = replaceNullValue_SAC(tup)\n",
    "    tup = replaceNullValue_SIC(tup)\n",
    "    tup = replaceNullValue_ToD(tup)\n",
    "    tup = replaceNullValue_TN(tup) \n",
    "    tup = replaceNullValue_THETA(tup)\n",
    "    tup = replaceNullValue_RHO(tup)\n",
    "    tup = replaceNullValue_FL(tup)\n",
    "    tup = replaceNullValue_CGS(tup)\n",
    "    tup = replaceNullValue_CHdg(RemoveQuoteCHdg(tup))\n",
    "    return tup\n",
    "    \n",
    "def main_clean(rdd):\n",
    "    \n",
    "    #header = rdd.first()\n",
    "    #rdd = rdd.filter(lambda line: line != header)\n",
    "    #rdd = rdd.map(lambda tup: RemoveStrangeTupleLen(tup))\\\n",
    "    #         .filter(lambda tup: tup!=None)\n",
    "             \n",
    "    rdd = rdd.map(lambda tup: cleaning(tup))\n",
    "    return(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT IN THE DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first need to import types (e.g. StructType, StructField, IntegerType, etc.)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def getlistavion(): \n",
    "    QUERY = 'SELECT DISTINCT(TID) FROM global_temp.traffic'\n",
    "    return spark.sql(QUERY).toPandas()\n",
    "\n",
    "def main_db(rdd): \n",
    "    global traffic_df_explicit, trafficSchema, spark  \n",
    "    \n",
    "    traffic_df_explicit_aux = spark.createDataFrame(rdd, trafficSchema)\n",
    "    traffic_df_explicit = traffic_df_explicit.unionAll(traffic_df_explicit_aux)       \n",
    "    \n",
    "    traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "    \n",
    "    traffic_df_explicit.cache()\n",
    "    \n",
    "    #Queries\n",
    "    #spark.sql(\"select TID, DST, COUNT(*) from global_temp.traffic WHERE TID != '' GROUP BY TID, DST ORDER BY COUNT(*) DESC\").show()\n",
    "    #spark.sql(\"select * from global_temp.traffic\").show()\n",
    "\n",
    "#main_db(rdd_traffic_clean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPacketsByPlane(tid):\n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    QUERY = 'SELECT * FROM global_temp.traffic \\\n",
    "            WHERE TID LIKE ' + str(TID)\n",
    "    #print(QUERY)\n",
    "    return spark.sql(QUERY).toPandas()\n",
    "\n",
    "def dictRadarsByAvion(TID):\n",
    "    df_temp = getPacketsByPlane(TID)\n",
    "    list_radar = list(df_temp.groupby('DST').size().index)\n",
    "    dictRadar = {}\n",
    "    for dst in list_radar:\n",
    "        #dictRadar[dst] = filterByPlaneAndRadar(TID, dst) ##TOO MUCH TIME\n",
    "        dictRadar[dst] = df_temp[df_temp['DST'] == dst] \n",
    "    return dictRadar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictRadarsByAvion('TVF168')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterByPlaneAndRadar(tid, dst):\n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    DST = '\\'%' + dst + '%\\'' \n",
    "    QUERY = 'SELECT * FROM global_temp.traffic WHERE TID LIKE ' + str(TID) + ' AND DST LIKE ' +  str(DST) \n",
    "    return spark.sql(QUERY).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSup20ForTS(tid, dst): \n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    DST = '\\'%' + dst + '%\\'' \n",
    "    QUERY = 'SELECT * FROM global_temp.traffic WHERE TID LIKE ' + str(TID) + ' AND DST LIKE ' +  str(DST) + ' AND CGS IS NOT NULL' \n",
    "    return spark.sql(QUERY).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countSup20ForTS('DSO05LM', '01:00:5e:50:01:42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#if len(countSup20ForTS('KLM587', '01:00:5e:50:00:0a')) < 20: \n",
    "#    return \"TOO MANY VALUES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import stumpy\n",
    "import numpy.testing as npt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries(df):   \n",
    "    \n",
    "    #if len(df) < 20: \n",
    "    #    return print(\"NOT ENOUGH VALUES\")\n",
    "    \n",
    "    '''\n",
    "    elif len(df) == 10:    \n",
    "        #--STREAM--\n",
    "        #initialisation \n",
    "        stream = stumpy.stumpi(T, m)\n",
    "    else:\n",
    "        #update t\n",
    "        stream.update(l)\n",
    "        #recup les résultats\n",
    "        ts = pd.Series(df_time_series_radar_avion[var].values, index = df_time_series_radar_avion['TS'])\n",
    "        ts.plot(color='r')\n",
    "        \n",
    "        ts_for = pd.Series(stream.P_, index = df_time_series_radar_avion['TS'] )\n",
    "        plt.show()\n",
    "    '''   \n",
    "\n",
    "    #--BATCH--\n",
    "    #matrix profile \n",
    "    #mp = stumpy.stump(T, m)\n",
    "\n",
    "    #recup les résultats\n",
    "    #P_full = mp[:, 0]\n",
    "    #Ifull = mp[:, 1]\n",
    "   \n",
    "    #df_time_series_radar_avion = df.copy()\n",
    "    #df_time_series_radar_avion['TS_DATE'] = df_time_series_radar_avion['TS'].copy()\n",
    "    \n",
    "    #for i in range(len(df_time_series_radar_avion['TS'])):\n",
    "    #    df_time_series_radar_avion['TS_DATE'][i] = datetime.datetime.fromtimestamp(df_time_series_radar_avion['TS'][i].astype('int64'))\n",
    "    \n",
    "    #ts = pd.Series(df_time_series_radar_avion[var].values, index = df_time_series_radar_avion['TS_DATE'])\n",
    "    #ts.plot(color='r')\n",
    "    #plt.title('Time Series Plot for CGS Variable over time')\n",
    "    #plt.show()\n",
    "    \n",
    "    #result = seasonal_decompose(ts, model='additive', period=10)\n",
    "    #result.plot()\n",
    "    #plt.show()\n",
    "\n",
    "    #window = 20\n",
    "    #ma = ts.rolling(window).mean()\n",
    "    #mstd = ts.rolling(window).std()\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.plot(ts.index, ts, 'r')\n",
    "    #plt.plot(ma.index, ma, 'b')\n",
    "    #plt.fill_between(mstd.index, ma - 2 * mstd, ma + 2 * mstd, color='b', alpha=0.2)\n",
    "    \n",
    "    ###FBPROPHET###\n",
    "    \n",
    "    m = Prophet()\n",
    "    #m.fit(df_time_series_radar_avion[['TS_DATE','CGS']].rename(columns={'TS_DATE': 'ds', var: \"y\"}))\n",
    "    #m.fit(df_time_series_radar_avion[['TS_DATE','CGS']])\n",
    "    #m.fit(df[['ds','y']])\n",
    "    \n",
    "    m.fit(df)\n",
    "    future = m.make_future_dataframe(periods=2,freq='min')\n",
    "        #future.tail()\n",
    "    #print(len(future))\n",
    "    forecast = m.predict(future)\n",
    "    m.plot(forecast)\n",
    "        #m.plot_components(forecast)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    ##ARIMA/ARMA Models \n",
    "    \n",
    "    ts_diff = ts - ts.shift()\n",
    "    diff = ts_diff.dropna()\n",
    "    plt.figure()\n",
    "    plt.plot(diff)\n",
    "    plt.title('First Difference Time Series Plot')\n",
    "    plt.show()\n",
    "    mod = ARIMA(ts, order = (1, 1, 1), seasonal_order = (2,0,0,12)) #modèle en dur, à changer \n",
    "    results = mod.fit()\n",
    "    ts.plot()\n",
    "    plt.plot(results.fittedvalues[20:], color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    predVals = results.predict(len(ts.index), len(ts.index) + 200, typ='levels')\n",
    "    dict_for = {}\n",
    "    \n",
    "    temps = df_time_series_radar_avion['TS'][len(ts.index) - 1]\n",
    "    for i in range(len(ts.index), len(ts.index) + 200):\n",
    "        dict_for[datetime.datetime.fromtimestamp(int(temps))] = predVals[i]\n",
    "        temps = temps + round(df_time_series_radar_avion['TS'][len(ts.index) - 1] - df_time_series_radar_avion['TS'][len(ts.index) - 2])\n",
    "    \n",
    "    ts.plot()\n",
    "    plt.plot(results.fittedvalues[20:], color='red')\n",
    "    plt.plot(pd.Series(dict_for), color='orange')\n",
    "    plt.show()\n",
    "    \n",
    "    window_for = 20\n",
    "    ma_for = pd.Series(dict_for).rolling(window).mean()\n",
    "    mstd_for = pd.Series(dict_for).rolling(window).std()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ts.index, ts, 'b')\n",
    "    plt.plot(results.fittedvalues[20:], color='red')\n",
    "    plt.plot(pd.Series(dict_for), color='orange')\n",
    "    plt.plot(ma_for.index, ma_for, 'b')\n",
    "    plt.fill_between(mstd_for.index, ma_for - 2 * mstd_for, ma_for + 2 * mstd_for, color='g', alpha=1)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "traffic_df_explicit = spark.createDataFrame(spark.sparkContext.emptyRDD(),trafficSchema)\n",
    "traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "\n",
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-12:00:00/2019-05-04-16:00:00', stream=True)\n",
    "\n",
    "#ype(response)\n",
    "i = 0\n",
    "\n",
    "list_aux = [] \n",
    "\n",
    "for data in response.iter_lines():\n",
    "    #print(data.decode(\"UTF-8\"))  \n",
    "    #print(str(data)[1:])\n",
    "    i = i + 1\n",
    "    #print(i)\n",
    "    #print([data.decode(\"UTF-8\").split(\",\")]) \n",
    "    #print(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    list_aux.append(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    if not(i % 1000):\n",
    "        #print(len(response.text))\n",
    "        print(i)\n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        \n",
    "        #print(rdd_traffic_clean.collect())\n",
    "        main_db(rdd_traffic_clean) \n",
    "        list_aux = []     \n",
    "        \n",
    "    #time series \n",
    "    \n",
    "    #if not(i % 4000):\n",
    "        #timeseries(countSup20ForTS('DSO05LM', '01:00:5e:50:01:42'), 'CGS') #how use it in real-time ? b\n",
    "        \n",
    "        ###timeseries(df, var)\n",
    "        ###timeseries(filterByPlaneAndRadar('TOM549','01:00:5e:50:00:26'), 'CGS')\n",
    "        ####FIN24Q\n",
    "           \n",
    "    #rdd_traffic = sc.parallelize([data.decode(\"UTF-8\").split(\",\")])\n",
    "    #rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "    #print(rdd_traffic_clean.collect())\n",
    "    #main_db(rdd_traffic_clean) \n",
    "    \n",
    "    #data viz \n",
    "    \n",
    "    #if (i == 1000): \n",
    "    #    viz(dictRadarsByAvion('EXS29RP'), 'EXS29RP')\n",
    "    \n",
    "    #data time series \n",
    "    \n",
    "    if (i==100000): break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the absence of an inbuit time series library in Spark (inaccessible in spark), the workaround could be Spark Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#type(traffic_df_explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from global_temp.traffic').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, struct, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(row):\n",
    "    \"\"\"Transform data from pyspark.sql.Row to python dict to be used in rdd.\"\"\"\n",
    "    data = row['data']\n",
    "    \n",
    "    # Transform [pyspark.sql.Dataframe.Row] -> [dict]\n",
    "    data_dicts = []\n",
    "    for d in data:\n",
    "        data_dicts.append(d.asDict())\n",
    "\n",
    "    # Convert into pandas dataframe for fbprophet\n",
    "    data = pd.DataFrame(data_dicts)\n",
    "    data['ds'] = pd.to_datetime(data['ds'], unit='s')\n",
    "\n",
    "    return {\n",
    "        'app' : 'bidule',\n",
    "        'data': data,\n",
    "        'metric' : 'bidule',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(d):\n",
    "    \"\"\"Split data into training and testing based on timestamp.\"\"\"\n",
    "    # Extract data from pd.Dataframe\n",
    "    data = d['data']\n",
    "\n",
    "    # Find max timestamp and extract timestamp for start of day\n",
    "    max_datetime = pd.to_datetime(max(data['ds']))\n",
    "    start_datetime = max_datetime.replace(hour=00, minute=00, second=00)\n",
    "\n",
    "    # Extract training data\n",
    "    train_data = data[data['ds'] < max_datetime]\n",
    "\n",
    "    # Account for zeros in data while still applying uniform transform\n",
    "    #train_data['y'] = train_data['y'].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "    # Assign train/test split\n",
    "    d['test_data'] = data.loc[(data['ds'] < start_datetime)\n",
    "                              & (data['ds'] <= max_datetime)]\n",
    "    d['train_data'] = train_data\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(d):\n",
    "    \"\"\"Create Prophet model using each input grid parameter set.\"\"\"\n",
    "    m = Prophet()\n",
    "    d['model'] = m\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(d):\n",
    "    \"\"\"Fit the model using the training data.\"\"\"\n",
    "    model = d['model']\n",
    "    train_data = d['train_data']\n",
    "    model.fit(train_data)\n",
    "    d['model'] = model\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecast(d):\n",
    "    \"\"\"Execute the forecast method on the model to make future predictions.\"\"\"\n",
    "    model = d['model']\n",
    "    future = model.make_future_dataframe(\n",
    "        periods=0, freq='min')\n",
    "    \n",
    "    forecast = model.predict(future)\n",
    "    d['forecast'] = forecast\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_scope(d):\n",
    "    \"\"\"Return a tuple (app + , + metric_type, {}).\"\"\"\n",
    "    return (\n",
    "        d['app'] + ',' + d['metric'],\n",
    "        {\n",
    "            'forecast': pd.concat([d['train_data']['y'],d['forecast']], axis=1),  \n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_predictions(d):\n",
    "    \"\"\"Flatten rdd into tuple which will be converted into a dataframe.Row.\n",
    "    Checks each float to see if it is a np datatype, since it could be None.\n",
    "    If it is an np datatype then it will convert to scalar python datatype\n",
    "    so that it can be persisted into a database, since most dont know how to\n",
    "    interpret np python datatypes.\n",
    "    \"\"\"\n",
    "    app_metric, data = d\n",
    "    app, metric = app_metric.split(',')\n",
    "    return [\n",
    "        (\n",
    "            p['ds'].strftime(\"%d-%b-%Y (%H:%M:%S)\"),\n",
    "            p['y'],\n",
    "            p['yhat'],\n",
    "            p['yhat_lower'],\n",
    "            p['yhat_upper'],\n",
    "        ) for i, p in data['forecast'].iterrows()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df_explicit.select(\n",
    "                    traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                    traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID == 'DSO05LM ' and DST == '01:00:5e:50:01:42'\")\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r : transform_data(r))\\\n",
    "                       .map(lambda d: partition_data(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model(d))\\\n",
    "                       .map(lambda d: train_model(d))\\\n",
    "                       .map(lambda d: make_forecast(d))\\\n",
    "                       .map(lambda d: reduce_data_scope(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions(d))\\\n",
    "                       .take(1)\n",
    "          \n",
    "#traffic_df_explicit.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_for = traffic_df_explicit.select(\n",
    "                    traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                    traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID == 'DSO05LM ' and DST == '01:00:5e:50:01:42'\")\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r : transform_data(r))\\\n",
    "                       .map(lambda d: partition_data(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model(d))\\\n",
    "                       .map(lambda d: train_model(d))\\\n",
    "                       .map(lambda d: make_forecast(d))\\\n",
    "                       .map(lambda d: reduce_data_scope(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions(d))\n",
    "          \n",
    "#type(traffic_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_for = StructType([\n",
    "        StructField(\"ds\", StringType(), True),\n",
    "        StructField(\"y\", FloatType(), True),\n",
    "        StructField(\"yhat\", FloatType(), True),\n",
    "        StructField(\"yhat_lower\", FloatType(), True),\n",
    "        StructField(\"yhat_upper\", FloatType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for = spark.createDataFrame(traffic_for, schema_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "traffic_df_explicit = spark.createDataFrame(spark.sparkContext.emptyRDD(),trafficSchema)\n",
    "traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "\n",
    "schema_for = StructType([\n",
    "        StructField(\"ds\", StringType(), True),\n",
    "        StructField(\"y\", FloatType(), True),\n",
    "        StructField(\"yhat\", FloatType(), True),\n",
    "        StructField(\"yhat_lower\", FloatType(), True),\n",
    "        StructField(\"yhat_upper\", FloatType(), True),\n",
    "    ])\n",
    "\n",
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-12:00:00/2019-05-04-16:00:00', stream=True)\n",
    "#ype(response)\n",
    "i = 0\n",
    "\n",
    "list_aux = [] \n",
    "cmpt_tram=0\n",
    "\n",
    "for data in response.iter_lines():\n",
    "    #print(data.decode(\"UTF-8\"))  \n",
    "    #print(str(data)[1:])\n",
    "    i = i + 1\n",
    "    #print(i)\n",
    "    #print([data.decode(\"UTF-8\").split(\",\")]) \n",
    "    #print(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    ligne = data.decode(\"UTF-8\").split(\",\")\n",
    "    list_aux.append(ligne)\n",
    "    \n",
    "       #prédire lorsque j'ai 5 nouveaux paquets pour un avion considere et un radar considere \n",
    "    #-> prediction \n",
    "    #ligne[2] : TID\n",
    "    #ligne[4] : DST\n",
    "    \n",
    "    #Pour l'avion et le radar considere\n",
    "    if('DSO05LM' in ligne[2] and '01:00:5e:50:01:42' in ligne[4]):\n",
    "        #compteur pour le nombre de tram\n",
    "        cmpt_tram += 1       \n",
    "        #print(compt_tram)\n",
    "        \n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        \n",
    "        #print(rdd_traffic_clean.collect())\n",
    "        main_db(rdd_traffic_clean) \n",
    "        list_aux = []  \n",
    "        \n",
    "        #envoie de la prédiction toutes les 5 trams\n",
    "        if(cmpt_tram==5):\n",
    "            #faire la prédiction \n",
    "            traffic_for = traffic_df_explicit.select(\n",
    "                    traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                    traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID == 'DSO05LM ' and DST == '01:00:5e:50:01:42'\")\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r : transform_data(r))\\\n",
    "                       .map(lambda d: partition_data(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model(d))\\\n",
    "                       .map(lambda d: train_model(d))\\\n",
    "                       .map(lambda d: make_forecast(d))\\\n",
    "                       .map(lambda d: reduce_data_scope(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions(d))\n",
    "        \n",
    "            traffic_for.cache()\n",
    "            df_for = spark.createDataFrame(traffic_for, schema_for)\n",
    "            df_for.show()\n",
    "            \n",
    "            #envoie de y et de la prédiction \n",
    "            \n",
    "            #Réinitialisation du compteur\n",
    "            cmpt_tram=0\n",
    "        else:\n",
    "            #Envoie de y\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    if(not(i%1000)):\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    list_aux.append(ligne)\n",
    "    \n",
    "    if not(i % 1000):\n",
    "        #print(len(response.text))\n",
    "        print(i)\n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        \n",
    "        #print(rdd_traffic_clean.collect())\n",
    "        main_db(rdd_traffic_clean) \n",
    "        list_aux = []     \n",
    "        \n",
    "    #time series \n",
    "    \n",
    "    if not(i % 4000):\n",
    "        traffic_for = traffic_df_explicit.select(\n",
    "                    traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                    traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID == 'DSO05LM ' and DST == '01:00:5e:50:01:42'\")\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r : transform_data(r))\\\n",
    "                       .map(lambda d: partition_data(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model(d))\\\n",
    "                       .map(lambda d: train_model(d))\\\n",
    "                       .map(lambda d: make_forecast(d))\\\n",
    "                       .map(lambda d: reduce_data_scope(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions(d))\n",
    "        \n",
    "        traffic_for.cache()\n",
    "        \n",
    "        df_for = spark.createDataFrame(traffic_for, schema_for)\n",
    "        \n",
    "        df_for.show()\n",
    "    '''\n",
    "        \n",
    "    if (i==100000): break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "traffic_df_explicit = spark.createDataFrame(spark.sparkContext.emptyRDD(),trafficSchema)\n",
    "traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "\n",
    "schema_for = StructType([\n",
    "        StructField(\"ds\", StringType(), True),\n",
    "        StructField(\"y\", FloatType(), True),\n",
    "        StructField(\"yhat\", FloatType(), True),\n",
    "        StructField(\"yhat_lower\", FloatType(), True),\n",
    "        StructField(\"yhat_upper\", FloatType(), True),\n",
    "    ])\n",
    "\n",
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-12:00:00/2019-05-04-16:00:00', stream=True)\n",
    "#ype(response)\n",
    "i = 0\n",
    "\n",
    "list_aux = [] \n",
    "cmpt_tram=0\n",
    "\n",
    "for data in response.iter_lines():\n",
    "    #print(data.decode(\"UTF-8\"))  \n",
    "    #print(str(data)[1:])\n",
    "    i = i + 1\n",
    "    #print(i)\n",
    "    #print([data.decode(\"UTF-8\").split(\",\")]) \n",
    "    #print(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    ligne = data.decode(\"UTF-8\").split(\",\")\n",
    "    \n",
    "       #prédire lorsque j'ai 5 nouveaux paquets pour un avion considere et un radar considere \n",
    "    #-> prediction \n",
    "    #ligne[2] : TID\n",
    "    #ligne[4] : DST\n",
    "    \n",
    "    #Pour l'avion et le radar considere\n",
    "    if('TOM97V' in ligne[2] and '01:00:5e:50:00:62' in ligne[4]):\n",
    "        #compteur pour le nombre de tram\n",
    "        cmpt_tram += 1\n",
    "        \n",
    "        #clean\n",
    "        \n",
    "        \n",
    "        if(cmpt_tram==5):\n",
    "            #faire la prédiction \n",
    "            \n",
    "            #et de la prédiction \n",
    "            \n",
    "            #Réinitialisation du compteur\n",
    "            cmpt_tram=0\n",
    "        else:\n",
    "            #Envoie de y\n",
    "        \n",
    "    \n",
    "    if(not i%500):\n",
    "        print(cmpt_tram)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    list_aux.append(ligne)\n",
    "    \n",
    "    if not(i % 1000):\n",
    "        #print(len(response.text))\n",
    "        print(i)\n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        \n",
    "        #print(rdd_traffic_clean.collect())\n",
    "        main_db(rdd_traffic_clean) \n",
    "        list_aux = []     \n",
    "        \n",
    "    #time series \n",
    "    \n",
    "    if not(i % 4000):\n",
    "        traffic_for = traffic_df_explicit.select(\n",
    "                    traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                    traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID == 'DSO05LM ' and DST == '01:00:5e:50:01:42'\")\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r : transform_data(r))\\\n",
    "                       .map(lambda d: partition_data(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model(d))\\\n",
    "                       .map(lambda d: train_model(d))\\\n",
    "                       .map(lambda d: make_forecast(d))\\\n",
    "                       .map(lambda d: reduce_data_scope(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions(d))\n",
    "        \n",
    "        traffic_for.cache()\n",
    "        \n",
    "        df_for = spark.createDataFrame(traffic_for, schema_for)\n",
    "        \n",
    "        df_for.show()\n",
    "    '''\n",
    "        \n",
    "    if (i==100000): break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple planes and multiple radars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select DST, TID, count(*) from global_temp.traffic group by DST, TID order by DST, count(*) DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, struct, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_m(row):\n",
    "    \"\"\"Transform data from pyspark.sql.Row to python dict to be used in rdd.\"\"\"\n",
    "    data = row['data']\n",
    "    tid = row['TID']\n",
    "    dst = row['DST']\n",
    "    \n",
    "    # Transform [pyspark.sql.Dataframe.Row] -> [dict]\n",
    "    data_dicts = []\n",
    "    for d in data:\n",
    "        data_dicts.append(d.asDict())\n",
    "\n",
    "    # Convert into pandas dataframe for fbprophet\n",
    "    data = pd.DataFrame(data_dicts)\n",
    "    data['ds'] = pd.to_datetime(data['ds'], unit='s')\n",
    "\n",
    "    return {\n",
    "        'tid' : tid,\n",
    "        'dst' : dst,\n",
    "        'data': data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data_m(d):\n",
    "    \"\"\"Split data into training and testing based on timestamp.\"\"\"\n",
    "    # Extract data from pd.Dataframe\n",
    "    data = d['data']\n",
    "\n",
    "    # Find max timestamp and extract timestamp for start of day\n",
    "    max_datetime = pd.to_datetime(max(data['ds']))\n",
    "    start_datetime = max_datetime.replace(hour=00, minute=00, second=00)\n",
    "\n",
    "    # Extract training data\n",
    "    train_data = data[data['ds'] <= max_datetime]\n",
    "\n",
    "    # Account for zeros in data while still applying uniform transform\n",
    "    #train_data['y'] = train_data['y'].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "    # Assign train/test split\n",
    "    d['test_data'] = data.loc[(data['ds'] < start_datetime)\n",
    "                              & (data['ds'] <= max_datetime)]\n",
    "    d['train_data'] = train_data\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_m(d):\n",
    "    \"\"\"Create Prophet model using each input grid parameter set.\"\"\"\n",
    "    m = Prophet()\n",
    "    d['model'] = m\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_m(d):\n",
    "    \"\"\"Fit the model using the training data.\"\"\"\n",
    "    model = d['model']\n",
    "    train_data = d['train_data']\n",
    "    model.fit(train_data)\n",
    "    d['model'] = model\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecast_m(d):\n",
    "    \"\"\"Execute the forecast method on the model to make future predictions.\"\"\"\n",
    "    model = d['model']\n",
    "    future = model.make_future_dataframe(\n",
    "        periods=2, freq='min')\n",
    "    \n",
    "    forecast = model.predict(future)\n",
    "    d['forecast'] = forecast\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_scope_m(d):\n",
    "    \"\"\"Return a tuple (app + , + metric_type, {}).\"\"\"\n",
    "    return (\n",
    "        d['tid'] + ',' + d['dst'],\n",
    "        {\n",
    "            'forecast': pd.concat([d['train_data']['y'],d['forecast']], axis=1),  \n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_predictions_m(d):\n",
    "    tid_dst, data = d\n",
    "    tid, dst = tid_dst.split(',')\n",
    "    return [\n",
    "        (\n",
    "            tid, \n",
    "            dst,\n",
    "            p['ds'].strftime(\"%d-%b-%Y (%H:%M:%S)\"),\n",
    "            p['y'],\n",
    "            p['yhat'],\n",
    "            p['yhat_lower'],\n",
    "            p['yhat_upper'],\n",
    "        ) for i, p in data['forecast'].iterrows()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select DST, TID, count(*) from global_temp.traffic group by DST, TID order by DST, count(*) DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "traffic_df_explicit.select(\n",
    "    traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "    traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                              .filter(\"TID like '%DSO05LM%' and DST like '%01:00:5e:50:01:42%'\")\\\n",
    "                              .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                              .rdd.map(lambda r : transform_data(r))\\\n",
    "                                  .map(lambda d: partition_data(d))\\\n",
    "                                  .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                                  .map(lambda d: create_model(d))\\\n",
    "                                  .map(lambda d: train_model(d))\\\n",
    "                                  .map(lambda d: make_forecast(d))\\\n",
    "                                  .map(lambda d: reduce_data_scope(d))\\\n",
    "                                  .flatMap(lambda d: expand_predictions(d))\\\n",
    "                                  .take(1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_for_m = traffic_df_explicit.select(\n",
    "                     traffic_df_explicit['TID'],\n",
    "                     traffic_df_explicit['DST'],                    \n",
    "                     traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                     traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"(TID like '%AFR15MD%' or TID like '%EWG7XB%') and DST like '%01:00:5e:50:00:06%'\")\\\n",
    "                   .groupBy('TID', 'DST')\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r: transform_data_m(r))\\\n",
    "                       .map(lambda d: partition_data_m(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model_m(d))\\\n",
    "                       .map(lambda d: train_model_m(d))\\\n",
    "                       .map(lambda d: make_forecast_m(d))\\\n",
    "                       .map(lambda d: reduce_data_scope_m(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions_m(d))\\\n",
    "                   #.rdd.map(lambda r : transform_data_m(r))\\\n",
    "                   #    .map(lambda d: partition_data_m(d))\\\n",
    "                   #    .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                   #    .map(lambda d: create_model_m(d))\\\n",
    "                      \n",
    "\n",
    "#01:00:5e:50:00:06                    \n",
    "#01:00:5e:50:00:06|EWG7XB  |      11|\n",
    "#|01:00:5e:50:00:06|HVN18   |      11|\n",
    "#|01:00:5e:50:00:06|EZY47UA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_for_m = StructType([\n",
    "        StructField(\"tid\", StringType(), True),\n",
    "        StructField(\"dst\", StringType(), True),\n",
    "        StructField(\"ds\", StringType(), True),\n",
    "        StructField(\"y\", FloatType(), True),\n",
    "        StructField(\"yhat\", FloatType(), True),\n",
    "        StructField(\"yhat_lower\", FloatType(), True),\n",
    "        StructField(\"yhat_upper\", FloatType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for = spark.createDataFrame(traffic_for_m, schema_for_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "traffic_df_explicit = spark.createDataFrame(spark.sparkContext.emptyRDD(),trafficSchema)\n",
    "traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "\n",
    "schema_for_m = StructType([\n",
    "        StructField(\"tid\", StringType(), True),\n",
    "        StructField(\"dst\", StringType(), True),\n",
    "        StructField(\"ds\", StringType(), True),\n",
    "        StructField(\"y\", FloatType(), True),\n",
    "        StructField(\"yhat\", FloatType(), True),\n",
    "        StructField(\"yhat_lower\", FloatType(), True),\n",
    "        StructField(\"yhat_upper\", FloatType(), True),\n",
    "    ])\n",
    "\n",
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-12:00:00/2019-05-04-16:00:00', stream=True)\n",
    "#ype(response)\n",
    "i = 0\n",
    "\n",
    "list_aux = [] \n",
    "\n",
    "for data in response.iter_lines():\n",
    "    #print(data.decode(\"UTF-8\"))  \n",
    "    #print(str(data)[1:])\n",
    "    i = i + 1\n",
    "    #print(i)\n",
    "    #print([data.decode(\"UTF-8\").split(\",\")]) \n",
    "    #print(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    list_aux.append(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    if not(i % 1000):\n",
    "        #print(len(response.text))\n",
    "        print(i)\n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        \n",
    "        #print(rdd_traffic_clean.collect())\n",
    "        main_db(rdd_traffic_clean) \n",
    "        list_aux = []     \n",
    "        \n",
    "    #time series \n",
    "    \n",
    "    if not(i % 4000):\n",
    "        traffic_for_m = traffic_df_explicit.select(\n",
    "                     traffic_df_explicit['TID'],\n",
    "                     traffic_df_explicit['DST'],                    \n",
    "                     traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                     traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID like '%AFR15MD%' and DST like '%01:00:5e:50:00:06%'\")\\\n",
    "                   .groupBy('TID', 'DST')\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r: transform_data_m(r))\\\n",
    "                       .map(lambda d: partition_data_m(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model_m(d))\\\n",
    "                       .map(lambda d: train_model_m(d))\\\n",
    "                       .map(lambda d: make_forecast_m(d))\\\n",
    "                       .map(lambda d: reduce_data_scope_m(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions_m(d))\\\n",
    "        \n",
    "        traffic_for_m.cache()\n",
    "        \n",
    "        df_for_m = spark.createDataFrame(traffic_for_m, schema_for_m)\n",
    "        \n",
    "        df_for_m.show()\n",
    "        \n",
    "    if (i==100000): break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976195.58', '01:00:5e:50:01:42', '8', '80', '48194.625', '1253', '349.738769531', '10.65234375', '79.5', '300.366210938', '212.135009766']\n",
      "        tid                dst                      ds          y yhat  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  04-May-2019 (13:23:16)  212.13501  NaN   \n",
      "\n",
      "  yhat_lower yhat_upper  \n",
      "0        NaN        NaN  \n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976199.49', '01:00:5e:50:01:42', '8', '80', '48198.5625', '1253', '348.711547852', '10.37890625', '79.5', '303.662109375', '195.485229492']\n",
      "        tid                dst                      ds           y yhat  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  04-May-2019 (13:23:19)  195.485229  NaN   \n",
      "\n",
      "  yhat_lower yhat_upper  \n",
      "0        NaN        NaN  \n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976203.44', '01:00:5e:50:01:42', '8', '80', '48202.515625', '1253', '347.750244141', '10.09375', '79.75', '305.419921875', '197.775878906']\n",
      "        tid                dst                      ds           y yhat  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  04-May-2019 (13:23:23)  197.775879  NaN   \n",
      "\n",
      "  yhat_lower yhat_upper  \n",
      "0        NaN        NaN  \n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976207.41', '01:00:5e:50:01:42', '8', '80', '48206.4765625', '1253', '346.849365234', '9.8046875', '80.0', '301.245117188', '196.391601562']\n",
      "        tid                dst                      ds           y yhat  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  04-May-2019 (13:23:27)  196.391602  NaN   \n",
      "\n",
      "  yhat_lower yhat_upper  \n",
      "0        NaN        NaN  \n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976211.36', '01:00:5e:50:01:42', '8', '80', '48210.421875', '1253', '345.992431641', '9.51171875', '81.25', '300.5859375', '193.172607422']\n",
      "+--------+-----------------+--------------------+---------+---------+----------+----------+\n",
      "|     tid|              dst|                  ds|        y|     yhat|yhat_lower|yhat_upper|\n",
      "+--------+-----------------+--------------------+---------+---------+----------+----------+\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...| 300.3662| 300.3662|  300.3662|  300.3662|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...| 303.6621| 303.6621|  303.6621|  303.6621|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|305.41992|305.41992| 305.41992| 305.41992|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|301.24512|301.24512| 301.24512| 301.24512|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|300.58594|300.58594| 300.58594| 300.58594|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|      NaN|290.69824|  72.43719| 518.08154|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|      NaN|280.81055| -347.5374|  887.6325|\n",
      "+--------+-----------------+--------------------+---------+---------+----------+----------+\n",
      "\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976215.31', '01:00:5e:50:01:42', '8', '80', '48214.375', '1253', '345.184936523', '9.2109375', '83.0', '303.22265625', '182.823486328']\n",
      "        tid                dst                      ds           y yhat  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  04-May-2019 (13:23:35)  182.823486  NaN   \n",
      "\n",
      "  yhat_lower yhat_upper  \n",
      "0        NaN        NaN  \n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976219.29', '01:00:5e:50:01:42', '8', '80', '48218.3359375', '1253', '344.421386719', '8.90625', '85.5', '306.298828125', '184.844970703']\n",
      "        tid                dst                      ds           y yhat  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  04-May-2019 (13:23:39)  184.844971  NaN   \n",
      "\n",
      "  yhat_lower yhat_upper  \n",
      "0        NaN        NaN  \n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976223.23', '01:00:5e:50:01:42', '8', '80', '48222.28125', '1253', '343.591918945', '8.60546875', '87.75', '307.397460938', '185.872192383']\n",
      "        tid                dst                      ds           y yhat  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  04-May-2019 (13:23:43)  185.872192  NaN   \n",
      "\n",
      "  yhat_lower yhat_upper  \n",
      "0        NaN        NaN  \n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976227.19', '01:00:5e:50:01:42', '8', '80', '48226.2265625', '1253', '342.680053711', '8.3125', '90.25', '304.541015625', '186.729125977']\n",
      "        tid                dst                      ds           y yhat  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  04-May-2019 (13:23:47)  186.729126  NaN   \n",
      "\n",
      "  yhat_lower yhat_upper  \n",
      "0        NaN        NaN  \n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976231.16', '01:00:5e:50:01:42', '8', '80', '48230.1875', '1253', '341.674804688', '8.0234375', '92.25', '301.904296875', '187.239990234']\n",
      "+--------+-----------------+--------------------+---------+---------+----------+----------+\n",
      "|     tid|              dst|                  ds|        y|     yhat|yhat_lower|yhat_upper|\n",
      "+--------+-----------------+--------------------+---------+---------+----------+----------+\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...| 300.3662|300.36682| 300.33005|  300.4009|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...| 303.6621| 303.6615| 303.62708| 303.69702|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|305.41992|  305.419|  305.3846| 305.45718|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|301.24512|301.24603| 301.20987|  301.2828|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|300.58594|300.58594| 300.55145| 300.62418|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|303.22266|303.22357| 303.18872|  303.2595|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|306.29883| 306.2979|  306.2655| 306.33636|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|307.39746| 307.3606| 307.32465| 307.39636|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|304.54102|304.61426| 304.57748|  304.6531|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...| 301.9043|301.86792| 301.82938| 301.90344|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|      NaN| 260.6726| 93.234215| 414.04446|\n",
      "|DSO05LM |01:00:5e:50:01:42|04-May-2019 (13:2...|      NaN|219.47733|-261.91678|  662.6953|\n",
      "+--------+-----------------+--------------------+---------+---------+----------+----------+\n",
      "\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976235.12', '01:00:5e:50:01:42', '8', '80', '48234.125', '1253', '340.532226562', '7.734375', '94.25', '305.419921875', '188.228759766']\n",
      "        tid                dst                      ds          y yhat  \\\n",
      "0  DSO05LM   01:00:5e:50:01:42  04-May-2019 (13:23:55)  188.22876  NaN   \n",
      "\n",
      "  yhat_lower yhat_upper  \n",
      "0        NaN        NaN  \n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "['00:23:9c:c1:94:5f', '48', 'DSO05LM ', '1556976239.08', '01:00:5e:50:01:42', '8', '80', '48238.078125', '1253', '339.307250977', '7.453125', '96.25', '306.079101562', '188.349609375']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f8c1176e3d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#print(rdd_traffic_clean.collect())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mmain_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd_traffic_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m#faire un show pour un envoi en temps réel à la base de données sql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a52ac7fe3c7a>\u001b[0m in \u001b[0;36mmain_db\u001b[0;34m(rdd)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mtraffic_df_explicit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrafficSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtraffic_df_explicit_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrafficSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtraffic_df_explicit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraffic_df_explicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraffic_df_explicit_aux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    603\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    604\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_to_java_object_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \"\"\"\n\u001b[1;32m   2382\u001b[0m         \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonToJava\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcountApprox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2626\u001b[0m             \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2628\u001b[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0m\u001b[1;32m   2629\u001b[0m                                       self._jrdd_deserializer, profiler)\n\u001b[1;32m   2630\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2516\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2517\u001b[0m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2518\u001b[0;31m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[0m\u001b[1;32m   2519\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;31m# TODO Refactor to use a mixin shared by JavaMember and JavaClass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_converters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_converters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1538\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_converters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m                         \u001b[0mtemp_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mArrayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"java.util.ArrayList\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mjava_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mjava_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1569\u001b[0m             answer, self._gateway_client, None, self._fqn)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "traffic_df_explicit = spark.createDataFrame(spark.sparkContext.emptyRDD(),trafficSchema)\n",
    "traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "\n",
    "schema_for_m = StructType([\n",
    "        StructField(\"tid\", StringType(), True),\n",
    "        StructField(\"dst\", StringType(), True),\n",
    "        StructField(\"ds\", StringType(), True),\n",
    "        StructField(\"y\", FloatType(), True),\n",
    "        StructField(\"yhat\", FloatType(), True),\n",
    "        StructField(\"yhat_lower\", FloatType(), True),\n",
    "        StructField(\"yhat_upper\", FloatType(), True),\n",
    "    ])\n",
    "    \n",
    "list_aux = [] \n",
    "cmpt_tram = 0        \n",
    "\n",
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-12:00:00/2019-05-04-16:00:00', stream=True)\n",
    "#type(response)\n",
    "\n",
    "i = 0\n",
    "for data in response.iter_lines():\n",
    "    #print(data.decode(\"UTF-8\"))  \n",
    "    #print(str(data)[1:])\n",
    "    i = i + 1\n",
    "    #print(i)\n",
    "    #print([data.decode(\"UTF-8\").split(\",\")]) \n",
    "    #print(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    ligne = data.decode(\"UTF-8\").split(\",\")\n",
    "    list_aux.append(ligne)\n",
    "    \n",
    "      \n",
    "    #prédire lorsque j'ai 5 nouveaux paquets pour un avion considere et un radar considere \n",
    "    #-> prediction \n",
    "    #ligne[2] : TID\n",
    "    #ligne[4] : DST\n",
    "    \n",
    "    #Pour l'avion et le radar considere\n",
    "    if('DSO05LM' in ligne[2] and '01:00:5e:50:01:42' in ligne[4]):\n",
    "        #compteur pour le nombre de tram   \n",
    "        print(ligne)\n",
    "        \n",
    "        #print(compt_tram)\n",
    "        \n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        \n",
    "        #print(rdd_traffic_clean.collect())\n",
    "        main_db(rdd_traffic_clean) \n",
    "        \n",
    "        #faire un show pour un envoi en temps réel à la base de données sql\n",
    "        \n",
    "        cmpt_tram += 1 \n",
    "        list_aux = []     \n",
    "        \n",
    "    #time series \n",
    "       \n",
    "        #envoie de la prédiction toutes les 5 trams\n",
    "        if(cmpt_tram==5):\n",
    "            #faire la prédiction \n",
    "            traffic_for_m = traffic_df_explicit.select(\n",
    "                     traffic_df_explicit['TID'],\n",
    "                     traffic_df_explicit['DST'],                    \n",
    "                     traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                     traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID like '%DSO05LM%' and DST like '%01:00:5e:50:01:42%'\")\\\n",
    "                   .groupBy('TID', 'DST')\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r: transform_data_m(r))\\\n",
    "                       .map(lambda d: partition_data_m(d))\\\n",
    "                       .filter(lambda d: len(d['train_data']) > 2)\\\n",
    "                       .map(lambda d: create_model_m(d))\\\n",
    "                       .map(lambda d: train_model_m(d))\\\n",
    "                       .map(lambda d: make_forecast_m(d))\\\n",
    "                       .map(lambda d: reduce_data_scope_m(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions_m(d))\\\n",
    "        \n",
    "            traffic_for_m.cache()\n",
    "        \n",
    "            df_for_m = spark.createDataFrame(traffic_for_m, schema_for_m)\n",
    "        \n",
    "            df_for_m.show()\n",
    "            \n",
    "            #envoie de y et de la prédiction \n",
    "            \n",
    "            #Réinitialisation du compteur\n",
    "            cmpt_tram=0\n",
    "        else:\n",
    "            #Envoie de y\n",
    "            \n",
    "            #clean et envoi de la ligne à la volée\n",
    "        \n",
    "            tid = ligne[2]\n",
    "            dst = ligne[4]\n",
    "            ds = pd.to_datetime(float(round(float(ligne[3]))), unit='s').strftime(\"%d-%b-%Y (%H:%M:%S)\")\n",
    "            y = float(ligne[13])\n",
    "            yhat = 'NaN'\n",
    "            yhat_lower = 'NaN'\n",
    "            yhat_upper = 'NaN'\n",
    "        \n",
    "            d = {'tid': [tid], 'dst': [dst], 'ds': [ds], 'y': [y], 'yhat': [yhat], \n",
    "             'yhat_lower': [yhat_lower], 'yhat_upper': [yhat_upper]}\n",
    "        \n",
    "            #spark.createDataFrame(traffic_for_m, schema_for_m).show()\n",
    "        \n",
    "            print(pd.DataFrame(data=d))\n",
    "    \n",
    "    if(not(i%1000)):\n",
    "        print(i)\n",
    "        \n",
    "    if (i==100000): break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######## "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 choses : \n",
    "-> Rajouter des nulls quand pas de y (pour pred) : Done\n",
    "-> Mise en forme des donnees (avec des nulls quand pas de pred realisée) : faire un clean direct et un envoi sans passer par Spark, un envoi et un clean à la volée : Done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:16)',\n",
       "  300.366210938,\n",
       "  301.9432051753301,\n",
       "  299.4996831172537,\n",
       "  304.3578307972929),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:19)',\n",
       "  303.662109375,\n",
       "  302.23578385392193,\n",
       "  299.8368285393827,\n",
       "  304.75964884276175),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:23)',\n",
       "  305.419921875,\n",
       "  302.62588876084345,\n",
       "  300.0737033971831,\n",
       "  305.37769154630035),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:27)',\n",
       "  301.245117188,\n",
       "  303.015993019659,\n",
       "  300.5175977625569,\n",
       "  305.5127219226727),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:31)',\n",
       "  300.5859375,\n",
       "  303.4060972731281,\n",
       "  300.75228698414617,\n",
       "  305.75120756802954),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:35)',\n",
       "  303.22265625,\n",
       "  303.814834587964,\n",
       "  301.23142192986376,\n",
       "  306.15416264321567),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:39)',\n",
       "  306.298828125,\n",
       "  304.223571981109,\n",
       "  301.8971216036121,\n",
       "  306.7573398874543),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:43)',\n",
       "  307.397460938,\n",
       "  304.6290300104111,\n",
       "  302.1670399143385,\n",
       "  307.1280808137313),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:47)',\n",
       "  304.541015625,\n",
       "  304.8029774900727,\n",
       "  302.40974145034943,\n",
       "  307.3164114348384),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:51)',\n",
       "  301.904296875,\n",
       "  304.97692494586795,\n",
       "  302.4631388216365,\n",
       "  307.4216134408404),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:55)',\n",
       "  305.419921875,\n",
       "  305.1508724016632,\n",
       "  302.64084929001706,\n",
       "  307.5851551932731),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:23:59)',\n",
       "  306.079101562,\n",
       "  305.32481985745846,\n",
       "  302.73614735476104,\n",
       "  307.6447540565593),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:24:59)',\n",
       "  nan,\n",
       "  307.9340316943876,\n",
       "  304.87719460507327,\n",
       "  310.8082842061272),\n",
       " ('DSO05LM ',\n",
       "  '01:00:5e:50:01:42',\n",
       "  '04-May-2019 (13:25:59)',\n",
       "  nan,\n",
       "  310.5432435313168,\n",
       "  305.34769010224534,\n",
       "  315.47632046681457)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_df_explicit.select(\n",
    "                     traffic_df_explicit['TID'],\n",
    "                     traffic_df_explicit['DST'],                    \n",
    "                     traffic_df_explicit['TS'].cast(IntegerType()).alias('ds'), \n",
    "                     traffic_df_explicit['CGS'].alias('y'))\\\n",
    "                   .filter(\"TID like '%DSO05LM%' and DST like '%01:00:5e:50:01:42%'\")\\\n",
    "                   .groupBy('TID', 'DST')\\\n",
    "                   .agg(collect_list(struct('ds', 'y')).alias('data'))\\\n",
    "                   .rdd.map(lambda r: transform_data_m(r))\\\n",
    "                       .map(lambda d: partition_data_m(d))\\\n",
    "                       .map(lambda d: create_model_m(d))\\\n",
    "                       .map(lambda d: train_model_m(d))\\\n",
    "                       .map(lambda d: make_forecast_m(d))\\\n",
    "                       .map(lambda d: reduce_data_scope_m(d))\\\n",
    "                       .flatMap(lambda d: expand_predictions_m(d))\\\n",
    "                       .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretized Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the batch duration is 2 seconds, then the data will be collected every 2 seconds and stored in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame and SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import mysql.connector as mconn\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "def create_database(database_name=\"test\"):\n",
    "    '''\n",
    "    #create a database using: CREATE DATABASE database_name;\n",
    "    #select a database using: USE database_name '''\n",
    "    conn = mconn.connect(host=\"localhost\", port=3306, user=\"root\", password=\"secret\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"CREATE DATABASE %s;\"%(database_name))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def show_available_databases():\n",
    "    conn = mconn.connect(host=\"localhost\", port=3306, user=\"root\", password=\"secret\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SHOW DATABASES;\")\n",
    "    databases = cur.fetchall()\n",
    "    for database in databases:\n",
    "        print(database)\n",
    "    conn.close()\n",
    "\n",
    "def create_table(table_name=\"tabl1\", database_name=\"test\"):\n",
    "    conn = mconn.connect(host=\"localhost\", port=3306, user=\"root\", password=\"secret\", database=database_name)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS %s (x INT, y FLOAT, y_low FLOAT, y_upp FLOAT);\"%(table_name))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def drop_table(table_name=\"tabl1\", database_name=\"test\"):\n",
    "    conn = mconn.connect(host=\"localhost\", port=3306, user=\"root\", password=\"secret\", database=database_name)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"DROP TABLE %s;\"%(table_name))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def draft_populate(table_name=\"tabl1\", database_name=\"test\"):\n",
    "    conn = mconn.connect(host=\"localhost\", port=3306, user=\"root\", password=\"secret\", database=database_name)\n",
    "    cur = conn.cursor()\n",
    "    with open(\"pourKaderEtCharles.csv\", \"r\") as f:\n",
    "        next(f)\n",
    "        for line_ in f:\n",
    "            line = line_.replace(\"\\n\", \"\")\n",
    "            _,y,ds,yhat_lower,yhat_upper,TID,DST = line.split(\",\")\n",
    "            t = datetime.strptime(ds,\"%Y-%m-%d %H:%M:%S\")\n",
    "            # print(\"==%s==\"%(y))\n",
    "            cur.execute(\"INSERT INTO %s VALUES (%s, %s, %s, %s);\"%(table_name,(t-datetime(1970,1,1)).total_seconds(), float(y), float(yhat_lower), float(yhat_upper)))\n",
    "            conn.commit()\n",
    "            sleep(2)\n",
    "        f.close()\n",
    "    conn.close()\n",
    "\n",
    "def query_from_table(table_name=\"tabl1\", database_name=\"test\"):\n",
    "    conn = mconn.connect(host=\"localhost\", port=3306, user=\"root\", password=\"secret\", database=database_name)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT * FROM %s;\"%(table_name))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "\n",
    "# create_database()\n",
    "# show_available_databases()\n",
    "# create_table()\n",
    "draft_populate()\n",
    "# query_from_table()\n",
    "# drop_table()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A chaque insert dans la base, une mise à jour des plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
