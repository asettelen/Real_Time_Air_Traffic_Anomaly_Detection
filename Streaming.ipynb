{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "#import datefinder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import re   \n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "#from statsmodels.tsa.arima_model import ARIMA, ARIMAResults\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import datetime\n",
    "from fbprophet import Prophet\n",
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Spark RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tuple with len !=14\n",
    "def RemoveStrangeTupleLen(tup):\n",
    "    if len(tup)==14:\n",
    "        return tup\n",
    "    \n",
    "#CLEANING MAC_ADDRESS_SRC FIELD\n",
    "def RemoveQuoteSrc(tup):\n",
    "    if tup[0][0]=='\"':\n",
    "        tup[0]=tup[0][1:]\n",
    "    return tup\n",
    "\n",
    "def RemoveWeirdAddress(tup):\n",
    "    if len(tup[0])>17:\n",
    "        tup[0]=None\n",
    "    return tup\n",
    "\n",
    "#CLEANING CAT FIELD\n",
    "def CATToInt(tup): \n",
    "    tup[1] = int(tup[1])\n",
    "    return tup\n",
    "\n",
    "#CLEANING TID FIELD\n",
    "def replaceNullValue_TID(tup):\n",
    "    if tup[2] != '' and tup[2] != None and tup[2] != 'NaN': \n",
    "        return tup\n",
    "    else: \n",
    "        tup[2] = ''\n",
    "        return tup\n",
    "    \n",
    "#CLEANING TS FIELD\n",
    "def TSToFloat(tup): \n",
    "    tup[3] = float(round(float(tup[3])))\n",
    "    return tup\n",
    "\n",
    "#CLEANING DST FIELD\n",
    "#None\n",
    "\n",
    "#CLEANING SAC FIELD\n",
    "def replaceNullValue_SAC(tup):\n",
    "    if tup[5] != '' and tup[5] != None and tup[5] != 'NaN': \n",
    "        tup[5] = float(tup[5])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[5] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING SIC FIELD\n",
    "def replaceNullValue_SIC(tup):\n",
    "    if tup[6] != '' and tup[6] != None and tup[6] != 'NaN': \n",
    "        tup[6] = float(tup[6])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[6] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING ToD FIELD\n",
    "def replaceNullValue_ToD(tup):\n",
    "    if tup[7] != '' and tup[7] != None and tup[7] != 'NaN': \n",
    "        tup[7] = float(tup[7])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[7] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING TN FIELD\n",
    "def replaceNullValue_TN(tup):\n",
    "    if tup[8] != '' and tup[8] != None and tup[8] != 'NaN': \n",
    "        tup[8] = float(tup[8])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[8] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING THETA FIELD\n",
    "def replaceNullValue_THETA(tup):\n",
    "    if tup[9] != '' and tup[9] != None and tup[9] != 'NaN': \n",
    "        tup[9] = float(tup[9])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[9] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING RHO FIELD\n",
    "def replaceNullValue_RHO(tup):\n",
    "    if tup[10] != '' and tup[10] != None and tup[10] != 'NaN': \n",
    "        tup[10] = float(tup[10])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[10] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING FL FIELD \n",
    "def replaceNullValue_FL(tup):\n",
    "    if tup[11] != '' and tup[11] != None and tup[11] != 'NaN': \n",
    "        tup[11] = float(tup[11])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[11] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING CGS FIELD\n",
    "def replaceNullValue_CGS(tup):\n",
    "    if tup[12] != '' and tup[12] != None and tup[12] != 'NaN': \n",
    "        tup[12] = float(tup[12])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[12] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING CHdg FIELD\n",
    "    \n",
    "def replaceNullValue_CHdg(tup):\n",
    "    if tup[13] != '' and tup[13] != None and tup[13] != 'NaN': \n",
    "        tup[13] = float(tup[13])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[13] = None\n",
    "        return tup\n",
    "    \n",
    "def RemoveQuoteCHdg(tup):\n",
    "    if tup[13][-1]=='\"':\n",
    "        tup[13]=tup[13][:-1]\n",
    "    return tup\n",
    "\n",
    "\n",
    "def cleaning(tup):\n",
    "    tup = RemoveQuoteSrc(tup)\n",
    "    tup = RemoveWeirdAddress(tup)\n",
    "    tup = CATToInt(tup)\n",
    "    tup = replaceNullValue_TID(tup)\n",
    "    tup = TSToFloat(tup)\n",
    "    tup = replaceNullValue_SAC(tup)\n",
    "    tup = replaceNullValue_SIC(tup)\n",
    "    tup = replaceNullValue_ToD(tup)\n",
    "    tup = replaceNullValue_TN(tup) \n",
    "    tup = replaceNullValue_THETA(tup)\n",
    "    tup = replaceNullValue_RHO(tup)\n",
    "    tup = replaceNullValue_FL(tup)\n",
    "    tup = replaceNullValue_CGS(tup)\n",
    "    tup = replaceNullValue_CHdg(RemoveQuoteCHdg(tup))\n",
    "    return tup\n",
    "    \n",
    "def main_clean(rdd):\n",
    "    \n",
    "    #header = rdd.first()\n",
    "    #rdd = rdd.filter(lambda line: line != header)\n",
    "    #rdd = rdd.map(lambda tup: RemoveStrangeTupleLen(tup))\\\n",
    "    #         .filter(lambda tup: tup!=None)\n",
    "             \n",
    "    rdd = rdd.map(lambda tup: cleaning(tup))\n",
    "    return(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def cleaning(tup):\n",
    "    if tup[0][0]=='\"':tup[0]=tup[0][1:]        \n",
    "    \n",
    "    if len(tup[0])>17:tup[0]=None\n",
    "    \n",
    "    tup[1] = int(tup[1])\n",
    "\n",
    "    if tup[2] != '' and tup[2] != None and tup[2] != 'NaN': None\n",
    "    else: tup[2] = ''\n",
    "     \n",
    "    tup[3] = float(round(float(tup[3])))\n",
    "\n",
    "    if tup[5] != '' and tup[5] != None and tup[5] != 'NaN': tup[5] = float(tup[5])    \n",
    "    else: tup[5] = None\n",
    "\n",
    "    if tup[6] != '' and tup[6] != None and tup[6] != 'NaN': tup[6] = float(tup[6])\n",
    "    else: tup[6] = None\n",
    "    \n",
    "    if tup[7] != '' and tup[7] != None and tup[7] != 'NaN': tup[7] = float(tup[7])\n",
    "    else: tup[7] = None\n",
    "\n",
    "    if tup[8] != '' and tup[8] != None and tup[8] != 'NaN': tup[8] = float(tup[8])\n",
    "    else: tup[8] = None\n",
    "\n",
    "    if tup[9] != '' and tup[9] != None and tup[9] != 'NaN': tup[9] = float(tup[9])\n",
    "    else: tup[9] = None\n",
    "\n",
    "    if tup[10] != '' and tup[10] != None and tup[10] != 'NaN': tup[10] = float(tup[10])\n",
    "    else: tup[10] = None\n",
    "    \n",
    "    if tup[11] != '' and tup[11] != None and tup[11] != 'NaN': tup[11] = float(tup[11])\n",
    "    else: tup[11] = None\n",
    "\n",
    "    if tup[12] != '' and tup[12] != None and tup[12] != 'NaN': tup[12] = float(tup[12])\n",
    "    else: tup[12] = None\n",
    "\n",
    "    if tup[13] != '' and tup[13] != None and tup[13] != 'NaN': tup[13] = float(tup[13])\n",
    "    else: tup[13] = None\n",
    "    \n",
    "    if tup[13][-1]=='\"': tup[13]=tup[13][:-1]\n",
    "    \n",
    "    return tup\n",
    "    \n",
    "def main_clean(rdd):\n",
    "    rdd = rdd.map(lambda tup: cleaning(tup))\n",
    "    return(rdd)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT IN THE DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first need to import types (e.g. StructType, StructField, IntegerType, etc.)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def getlistavion(): \n",
    "    QUERY = 'SELECT DISTINCT(TID) FROM global_temp.traffic'\n",
    "    return spark.sql(QUERY).toPandas()\n",
    "\n",
    "def main_db(rdd): \n",
    "    global traffic_df_explicit, trafficSchema, spark  \n",
    "    \n",
    "    traffic_df_explicit_aux = spark.createDataFrame(rdd, trafficSchema)\n",
    "    traffic_df_explicit = traffic_df_explicit.unionAll(traffic_df_explicit_aux)       \n",
    "    \n",
    "    traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "    \n",
    "    traffic_df_explicit.cache()\n",
    "    \n",
    "    #Queries\n",
    "    #print(getlistavion())\n",
    "    #spark.sql(\"select * from global_temp.traffic\").show()\n",
    "\n",
    "#main_db(rdd_traffic_clean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from global_temp.traffic\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPacketsByPlane(tid):\n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    QUERY = 'SELECT * FROM global_temp.traffic \\\n",
    "            WHERE TID LIKE ' + str(TID)\n",
    "    #print(QUERY)\n",
    "    return spark.sql(QUERY).toPandas()\n",
    "\n",
    "def dictRadarsByAvion(TID):\n",
    "    df_temp = getPacketsByPlane(TID)\n",
    "    list_radar = list(df_temp.groupby('DST').size().index)\n",
    "    dictRadar = {}\n",
    "    for dst in list_radar:\n",
    "        #dictRadar[dst] = filterByPlaneAndRadar(TID, dst) ##TOO MUCH TIME\n",
    "        dictRadar[dst] = df_temp[df_temp['DST'] == dst] \n",
    "    return dictRadar\n",
    "\n",
    "import colorsys\n",
    "import random\n",
    "colorsys.hsv_to_rgb(359,100,100)\n",
    "def randomColor(i, m):\n",
    "    return rgb_to_hex(colorsys.hsv_to_rgb(float(i) / float(m), 1, 1))\n",
    "def rgb_to_hex(rgb):\n",
    "    rgbInt = (int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255))\n",
    "    return '#%02x%02x%02x' % rgbInt\n",
    "\n",
    "def plotRadar(dictRadar, var=None):\n",
    "        if var == None:\n",
    "            for var in list(dictRadar[list(dictRadar.keys())[0]].columns):\n",
    "                i, m = 0, len(dictRadar.keys())\n",
    "                for dst in dictRadar.keys():\n",
    "                    plt.plot(dictRadar[dst]['TS'], dictRadar[dst][var], randomColor(i, m))\n",
    "                    i = i + 1\n",
    "                plt.xlabel('time(s)')\n",
    "                plt.ylabel(var)\n",
    "                plt.show()            \n",
    "        else:\n",
    "            i, m = 0, len(dictRadar.keys())\n",
    "            for dst in dictRadar.keys():\n",
    "                plt.plot(dictRadar[dst]['TS'], dictRadar[dst][var], randomColor(i, m))\n",
    "                i = i + 1\n",
    "            plt.xlabel('time(s)')\n",
    "            plt.ylabel(var)\n",
    "            plt.show()  \n",
    "        \n",
    "        \n",
    "def viz(dictRadar, TID):\n",
    "    \n",
    "    plotRadar(dictRadar)\n",
    "    \n",
    "    sns.pairplot(data=getPacketsByPlane(TID), vars=['SAC', 'SIC', 'ToD', 'TN', 'THETA', 'RHO', 'FL', 'CGS', 'CHdg'])\n",
    "    plt.show()\n",
    "    \n",
    "    sns.heatmap(data=getPacketsByPlane(TID)[['SIC', 'ToD', 'TN', 'THETA', 'FL', 'CGS', 'CHdg']].corr(), annot = True, cmap = 'Reds')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterByPlaneAndRadar(tid, dst):\n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    DST = '\\'%' + dst + '%\\'' \n",
    "    QUERY = 'SELECT * FROM global_temp.traffic WHERE TID LIKE ' + str(TID) + ' AND DST LIKE ' +  str(DST) \n",
    "    return spark.sql(QUERY).toPandas()\n",
    "\n",
    "def plotOneRadarForOnePlane(df_radar_avion):\n",
    "    for var in df_radar_avion.columns:\n",
    "        plt.plot(df_radar_avion['TS'], df_radar_avion[var])\n",
    "        plt.xlabel('time(s)')\n",
    "        plt.ylabel(var)\n",
    "        plt.show() \n",
    "        \n",
    "def viz(df_radar_avion):\n",
    "    \n",
    "    plotOneRadarForOnePlane(df_radar_avion)\n",
    "    \n",
    "    sns.pairplot(data=df_radar_avion, vars=['SAC', 'SIC', 'ToD', 'TN', 'THETA', 'RHO', 'FL', 'CGS', 'CHdg'])\n",
    "    plt.show()\n",
    "    \n",
    "    sns.heatmap(data=df_radar_avion[['SIC', 'ToD', 'TN', 'THETA', 'FL', 'CGS', 'CHdg']].corr(), annot = True, cmap = 'Reds')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries(df, var): \n",
    "\n",
    "    df_time_series_radar_avion = df.copy()\n",
    "    df_time_series_radar_avion['TS_DATE'] = df_time_series_radar_avion['TS'].copy()\n",
    "    \n",
    "    for i in range(len(df_time_series_radar_avion['TS'])):\n",
    "        df_time_series_radar_avion['TS_DATE'][i] = datetime.datetime.fromtimestamp(df_time_series_radar_avion['TS'][i].astype('int64'))\n",
    "    \n",
    "    ts = pd.Series(df_time_series_radar_avion[var].values, index = df_time_series_radar_avion['TS_DATE'])\n",
    "    ts.plot(color='r')\n",
    "    plt.title('Time Series Plot for CGS Variable over time')\n",
    "    plt.show()\n",
    "    \n",
    "    result = seasonal_decompose(ts, model='additive', period=10)\n",
    "    result.plot()\n",
    "    plt.show()\n",
    "\n",
    "    window = 20\n",
    "    ma = ts.rolling(window).mean()\n",
    "    mstd = ts.rolling(window).std()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ts.index, ts, 'r')\n",
    "    plt.plot(ma.index, ma, 'b')\n",
    "    plt.fill_between(mstd.index, ma - 2 * mstd, ma + 2 * mstd, color='b', alpha=0.2)\n",
    "    \n",
    "    ###Fbprophet###\n",
    "    \n",
    "    m = Prophet()\n",
    "    m.fit(df_time_series_radar_avion[['TS_DATE',var]].rename(columns={'TS_DATE': 'ds', var: \"y\"}))\n",
    "    future = m.make_future_dataframe(periods=20,freq='min')\n",
    "    future.tail()\n",
    "    forecast = m.predict(future)\n",
    "    m.plot(forecast)\n",
    "    #m.plot_components(forecast)\n",
    "    \n",
    "    ##ARIMA/ARMA Models \n",
    "    \n",
    "    ts_diff = ts - ts.shift()\n",
    "    diff = ts_diff.dropna()\n",
    "    plt.figure()\n",
    "    plt.plot(diff)\n",
    "    plt.title('First Difference Time Series Plot')\n",
    "    plt.show()\n",
    "    mod = ARIMA(ts, order = (1, 1, 1), seasonal_order = (2,0,0,12))\n",
    "    results = mod.fit()\n",
    "    ts.plot()\n",
    "    plt.plot(results.fittedvalues[20:], color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    predVals = results.predict(len(ts.index), len(ts.index) + 200, typ='levels')\n",
    "    dict_for = {}\n",
    "    \n",
    "    temps = df_time_series_radar_avion['TS'][len(ts.index) - 1]\n",
    "    for i in range(len(ts.index), len(ts.index) + 200):\n",
    "        dict_for[datetime.datetime.fromtimestamp(int(temps))] = predVals[i]\n",
    "        temps = temps + round(df_time_series_radar_avion['TS'][len(ts.index) - 1] - df_time_series_radar_avion['TS'][len(ts.index) - 2])\n",
    "    \n",
    "    ts.plot()\n",
    "    plt.plot(results.fittedvalues[20:], color='red')\n",
    "    plt.plot(pd.Series(dict_for), color='orange')\n",
    "    plt.show()\n",
    "    \n",
    "    window_for = 20\n",
    "    ma_for = pd.Series(dict_for).rolling(window).mean()\n",
    "    mstd_for = pd.Series(dict_for).rolling(window).std()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ts.index, ts, 'b')\n",
    "    plt.plot(results.fittedvalues[20:], color='red')\n",
    "    plt.plot(pd.Series(dict_for), color='orange')\n",
    "    plt.plot(ma_for.index, ma_for, 'b')\n",
    "    plt.fill_between(mstd_for.index, ma_for - 2 * mstd_for, ma_for + 2 * mstd_for, color='g', alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "traffic_df_explicit = spark.createDataFrame(spark.sparkContext.emptyRDD(),trafficSchema)\n",
    "traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "\n",
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-12:00:00/2019-05-04-16:00:00', stream=True)\n",
    "#ype(response)\n",
    "i = 0\n",
    "\n",
    "list_aux = [] \n",
    "\n",
    "for data in response.iter_lines():\n",
    "    #print(data.decode(\"UTF-8\"))  \n",
    "    #print(str(data)[1:])\n",
    "    i = i + 1\n",
    "    #print(i)\n",
    "    #print([data.decode(\"UTF-8\").split(\",\")]) \n",
    "    #print(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    list_aux.append(data.decode(\"UTF-8\").split(\",\"))\n",
    "    \n",
    "    if not(i % 10000):\n",
    "        print(i)\n",
    "        rdd_traffic = sc.parallelize(list_aux)\n",
    "        rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "        \n",
    "        #print(rdd_traffic_clean.collect())\n",
    "        main_db(rdd_traffic_clean) \n",
    "        list_aux = []\n",
    "        \n",
    "        #timeseries(df, var)\n",
    "        #timeseries(filterByPlaneAndRadar('TOM549','01:00:5e:50:00:26'), 'CGS')\n",
    "        #FIN24Q\n",
    "           \n",
    "    #rdd_traffic = sc.parallelize([data.decode(\"UTF-8\").split(\",\")])\n",
    "    #rdd_traffic_clean = main_clean(rdd_traffic)\n",
    "    #print(rdd_traffic_clean.collect())\n",
    "    #main_db(rdd_traffic_clean) \n",
    "    \n",
    "    #data viz \n",
    "    \n",
    "    #if (i == 1000): \n",
    "    #    viz(dictRadarsByAvion('EXS29RP'), 'EXS29RP')\n",
    "    \n",
    "    #data time series \n",
    "    \n",
    "    if (i==100000): break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(DISTINCT TID)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(DISTINCT TID)\n",
       "0                 1128"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(TID)) from global_temp.traffic\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select TID, COUNT(*) from global_temp.traffic GROUP BY TID\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictRadarsByAvion('EXS29RP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz(dictRadarsByAvion('EZY51AQ'), 'EZY51AQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc_stream = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc_stream, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text data received over a TCP socket connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"http://192.168.37.142\", 50005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating DStreams from files as input sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_stream = SparkContext(appName=\"PysparkStreaming\")\n",
    "ssc_stream = StreamingContext(sc_stream, 3)   #Streaming will execute in each 3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc_stream.textFileStream(\"test.txt\")  #'log/ mean directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"log/test.txt\")\n",
    "lines.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "        .map(lambda x: (x, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "counts.pprint()\n",
    "ssc_stream.start()\n",
    "ssc_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\"\"\"\n",
    "This is use for create streaming of text from txt files that creating dynamically \n",
    "from files.py code. This spark streaming will execute in each 3 seconds and It'll\n",
    "show number of words count from each files dynamically\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    sc = SparkContext(appName=\"PysparkStreaming\")\n",
    "    ssc = StreamingContext(sc, 3)   #Streaming will execute in each 3 seconds\n",
    "    lines = ssc.textFileStream('log/')  #'log/ mean directory name\n",
    "    counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "        .map(lambda x: (x, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "    counts.pprint()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame and SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A chaque insert dans la base, une mise à jour des plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
