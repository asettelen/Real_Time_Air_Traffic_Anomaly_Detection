{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Activus - Partie RDD & Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages and the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install asterix\n",
    "#pip install dpkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "#import datefinder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import re   \n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "#from statsmodels.tsa.arima_model import ARIMA, ARIMAResults\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import datetime\n",
    "from fbprophet import Prophet\n",
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Spark RDD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting & Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-00:00:00/2019-05-04-04:00:00', stream=True)\n",
    "#ype(response)\n",
    "for data in response.iter_lines():\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = requests.get('http://192.168.37.142:50005/stream/2019-08-04-00:00:00/2019-08-04-04:00:00')\n",
    "req = requests.get(!curl http://192.168.37.142:50005/stream/2019-05-04-00:00:00/2019-05-04-04:00:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(req)\n",
    "req.line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_df=pd.DataFrame(req[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for line in req_df[0]:\n",
    "    l=line.split(',')\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic = sc.textFile('test.csv') \\\n",
    "                .map(lambda line: line.split(\",\")) \\\n",
    "                .filter(lambda line: len(line)>1) \\\n",
    "                .map(lambda line: (line[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_traffic.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://192.168.37.142:50005/stream/2019-05-04-00:00:00/2019-05-04-04:00:00', stream=True)\n",
    "#ype(response)\n",
    "i = 0\n",
    "for data in response.iter_lines():\n",
    "    #print(data.decode(\"UTF-8\"))  \n",
    "    #print(str(data)[1:])\n",
    "    i = i + 1\n",
    "    #print(i)\n",
    "    \n",
    "    #print([data.decode(\"UTF-8\").split(\",\")])\n",
    "\n",
    "    rdd_traffic = sc.parallelize([data.decode(\"UTF-8\").split(\",\")])\n",
    "    \n",
    "    rdd_traffic_clean = main(rdd_traffic)\n",
    "    \n",
    "    print(rdd_traffic_clean.collect())\n",
    "    \n",
    "    if (i==100): break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress Cleaning Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tuple with len !=14\n",
    "def RemoveStrangeTupleLen(tup):\n",
    "    if len(tup)==14:\n",
    "        return tup\n",
    "    \n",
    "#CLEANING MAC_ADDRESS_SRC FIELD\n",
    "def RemoveQuoteSrc(tup):\n",
    "    if tup[0][0]=='\"':\n",
    "        tup[0]=tup[0][1:]\n",
    "    return tup\n",
    "\n",
    "def RemoveWeirdAddress(tup):\n",
    "    if len(tup[0])>17:\n",
    "        tup[0]=None\n",
    "    return tup\n",
    "\n",
    "#CLEANING CAT FIELD\n",
    "def CATToInt(tup): \n",
    "    tup[1] = int(tup[1])\n",
    "    return tup\n",
    "\n",
    "#CLEANING TID FIELD\n",
    "def replaceNullValue_TID(tup):\n",
    "    if tup[2] != '' and tup[2] != None and tup[2] != 'NaN': \n",
    "        return tup\n",
    "    else: \n",
    "        tup[2] = ''\n",
    "        return tup\n",
    "    \n",
    "#CLEANING TS FIELD\n",
    "def TSToFloat(tup): \n",
    "    tup[3] = float(round(float(tup[3])))\n",
    "    return tup\n",
    "\n",
    "#CLEANING DST FIELD\n",
    "#None\n",
    "\n",
    "#CLEANING SAC FIELD\n",
    "def replaceNullValue_SAC(tup):\n",
    "    if tup[5] != '' and tup[5] != None and tup[5] != 'NaN': \n",
    "        tup[5] = float(tup[5])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[5] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING SIC FIELD\n",
    "def replaceNullValue_SIC(tup):\n",
    "    if tup[6] != '' and tup[6] != None and tup[6] != 'NaN': \n",
    "        tup[6] = float(tup[6])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[6] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING ToD FIELD\n",
    "def replaceNullValue_ToD(tup):\n",
    "    if tup[7] != '' and tup[7] != None and tup[7] != 'NaN': \n",
    "        tup[7] = float(tup[7])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[7] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING TN FIELD\n",
    "def replaceNullValue_TN(tup):\n",
    "    if tup[8] != '' and tup[8] != None and tup[8] != 'NaN': \n",
    "        tup[8] = float(tup[8])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[8] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING THETA FIELD\n",
    "def replaceNullValue_THETA(tup):\n",
    "    if tup[9] != '' and tup[9] != None and tup[9] != 'NaN': \n",
    "        tup[9] = float(tup[9])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[9] = None\n",
    "        return tup\n",
    "\n",
    "#CLEANING RHO FIELD\n",
    "def replaceNullValue_RHO(tup):\n",
    "    if tup[10] != '' and tup[10] != None and tup[10] != 'NaN': \n",
    "        tup[10] = float(tup[10])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[10] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING FL FIELD \n",
    "def replaceNullValue_FL(tup):\n",
    "    if tup[11] != '' and tup[11] != None and tup[11] != 'NaN': \n",
    "        tup[11] = float(tup[11])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[11] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING CGS FIELD\n",
    "def replaceNullValue_CGS(tup):\n",
    "    if tup[12] != '' and tup[12] != None and tup[12] != 'NaN': \n",
    "        tup[12] = float(tup[12])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[12] = None\n",
    "        return tup\n",
    "    \n",
    "#CLEANING CHdg FIELD\n",
    "    \n",
    "def replaceNullValue_CHdg(tup):\n",
    "    if tup[13] != '' and tup[13] != None and tup[13] != 'NaN': \n",
    "        tup[13] = float(tup[13])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[13] = None\n",
    "        return tup\n",
    "    \n",
    "def RemoveQuoteCHdg(tup):\n",
    "    if tup[13][-1]=='\"':\n",
    "        tup[13]=tup[13][:-1]\n",
    "    return tup\n",
    "\n",
    "\n",
    "def cleaning(tup):\n",
    "    tup = RemoveQuoteSrc(tup)\n",
    "    tup = RemoveWeirdAddress(tup)\n",
    "    tup = CATToInt(tup)\n",
    "    tup = replaceNullValue_TID(tup)\n",
    "    tup = TSToFloat(tup)\n",
    "    tup = replaceNullValue_SAC(tup)\n",
    "    tup = replaceNullValue_SIC(tup)\n",
    "    tup = replaceNullValue_ToD(tup)\n",
    "    tup = replaceNullValue_TN(tup) \n",
    "    tup = replaceNullValue_THETA(tup)\n",
    "    tup = replaceNullValue_RHO(tup)\n",
    "    tup = replaceNullValue_FL(tup)\n",
    "    tup = replaceNullValue_CGS(tup)\n",
    "    tup = replaceNullValue_CHdg(RemoveQuoteCHdg(tup))\n",
    "    return tup\n",
    "    \n",
    "def main(rdd):\n",
    "    \n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda line: line != header)\n",
    "    rdd = rdd.map(lambda tup: RemoveStrangeTupleLen(tup))\\\n",
    "             .filter(lambda tup: tup!=None)\n",
    "             \n",
    "    rdd = rdd.map(lambda tup: cleaning(tup))\n",
    "    return(rdd)\n",
    "    \n",
    "rdd_traffic_clean = main(rdd_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic_clean.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic_clean.saveAsTextFile('test_rdd')\n",
    "#we have two files : because we have 2 partitions. \n",
    "#It means that all the work (transformation, action) wille be done \n",
    "#in parallel. \n",
    "#When we save, each file is saved differently\n",
    "#Each partition will directly saves its content \n",
    "#It is like we have 2 machines and each machine is saving its part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE THE RESULTING RDD TO AN EXTERNAL STORAGE SYSTEM (e.g. HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns names \n",
    "'''['SRC',\n",
    "  'CAT',\n",
    "  'TID',\n",
    "  'TS',\n",
    "  'DST',\n",
    "  'SAC',\n",
    "  'SIC',\n",
    "  'ToD',\n",
    "  'TN',\n",
    "  'THETA',\n",
    "  'RHO',\n",
    "  'FL',\n",
    "  'CGS',\n",
    "  'CHdg']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = rdd_traffic.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_traffic = rdd_traffic.filter(lambda line: line != header)\n",
    "rdd_traffic.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic.map(lambda tup: tup[0][0]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#group by len\n",
    "#rdd_traffic.map(lambda tup: (len(tup),1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tuple with len !=14\n",
    "def RemoveStrangeTupleLen(tup):\n",
    "    if len(tup)==14:\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic_clean = rdd_traffic.map(lambda tup: RemoveStrangeTupleLen(tup))\\\n",
    "                               .filter(lambda tup: tup!=None)\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (len(tup),1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()\n",
    "#def tupleToList(tup): \n",
    "#rdd_traffic.map(lambda tup: tupleToList(tup)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING MAC_ADDRESS_SRC FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAC_ADDRESS_SRC\n",
    "#rdd_traffic.filter(lambda tup: tup[0] == None or tup[0] == '' or tup[0] == 'NaN').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAC_ADDRESS_SRC\n",
    "#def RemoveQuoteMacAddr(tup):\n",
    "#    if tup[0][0]=='\"' and len(tup[0])>=1:\n",
    "#        tup[0]=tup[0][1:]\n",
    "#    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic_clean = rdd_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveQuoteSrc(tup):\n",
    "    if tup[0][0]=='\"':\n",
    "        tup[0]=tup[0][1:]\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Src\n",
    "rdd_traffic_clean=rdd_traffic_clean.map(lambda tup: RemoveQuoteSrc(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[0],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveWeirdAddress(tup):\n",
    "    if len(tup[0])>17:\n",
    "        tup[0]=None\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_traffic_clean=rdd_traffic_clean.map(lambda tup: RemoveWeirdAddress(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[0],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .map(lambda tup: tup[0] if tup[0] != None else None)\\\n",
    "#           .filter(lambda element: element != None)\\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[0],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING CAT FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#CAT\n",
    "#rdd_traffic.filter(lambda tup: tup[1] == None or tup[1] == '' or tup[1] == 'NaN').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CATToInt(tup): \n",
    "    tup[1] = int(tup[1])\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAT\n",
    "rdd_traffic_clean.map(lambda tup: CATToInt(tup)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING TID FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[2],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_TID(tup):\n",
    "    if tup[2] != '' and tup[2] != None and tup[2] != 'NaN': \n",
    "        return tup\n",
    "    else: \n",
    "        tup[2] = ''\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TID\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_TID(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[2],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING TS FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TS\n",
    "#rdd_traffic_clean.filter(lambda tup: tup[3] == None or tup[3] == '' or tup[3] == 'NaN').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSToFloat(tup): \n",
    "    tup[3] = float(round(float(tup[3])))\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: TSToFloat(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING DST FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[4],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#DST\n",
    "#rdd_traffic_clean.filter(lambda tup: tup[4] == None or tup[4] == '' or tup[4] == 'NaN').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING SAC FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[5],1)) \\\n",
    "#                 .reduceByKey(lambda x,y : x+y) \\\n",
    "#                 .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_SAC(tup):\n",
    "    if tup[5] != '' and tup[5] != None and tup[5] != 'NaN': \n",
    "        tup[5] = float(tup[5])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[5] = None\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAC\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_SAC(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING SIC FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[6],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_SIC(tup):\n",
    "    if tup[6] != '' and tup[6] != None and tup[6] != 'NaN': \n",
    "        tup[6] = float(tup[6])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[6] = None\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SIC\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_SIC(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING ToD FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToD\n",
    "#rdd_traffic_clean.filter(lambda tup: tup[7] == None or tup[7] == '' or tup[7] == 'NaN').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[7],1)) \\\n",
    "#                 .reduceByKey(lambda x,y : x+y) \\\n",
    "#                 .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_ToD(tup):\n",
    "    if tup[7] != '' and tup[7] != None and tup[7] != 'NaN': \n",
    "        tup[7] = float(tup[7])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[7] = None\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ToD\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_ToD(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING TN FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[8],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_TN(tup):\n",
    "    if tup[8] != '' and tup[8] != None and tup[8] != 'NaN': \n",
    "        tup[8] = float(tup[8])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[8] = None\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TN\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_TN(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING THETA FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[9],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_THETA(tup):\n",
    "    if tup[9] != '' and tup[9] != None and tup[9] != 'NaN': \n",
    "        tup[9] = float(tup[9])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[9] = None\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#THETA\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_THETA(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING RHO FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[10],1)) \\\n",
    "#                 .reduceByKey(lambda x,y : x+y) \\\n",
    "#                 .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_RHO(tup):\n",
    "    if tup[10] != '' and tup[10] != None and tup[10] != 'NaN': \n",
    "        tup[10] = float(tup[10])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[10] = None\n",
    "        \n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RHO\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_RHO(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING FL FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[11],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_FL(tup):\n",
    "    if tup[11] != '' and tup[11] != None and tup[11] != 'NaN': \n",
    "        tup[11] = float(tup[11])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[11] = None\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FL\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_FL(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING CGS FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[12],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_CGS(tup):\n",
    "    if tup[12] != '' and tup[12] != None and tup[12] != 'NaN': \n",
    "        tup[12] = float(tup[12])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[12] = None\n",
    "        return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CGS\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_CGS(tup))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: (tup[13],1)) \\\n",
    "#           .reduceByKey(lambda x,y : x+y) \\\n",
    "#           .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNullValue_CHdg(tup):\n",
    "    if tup[13] != '' and tup[13] != None and tup[13] != 'NaN': \n",
    "        tup[13] = float(tup[13])\n",
    "        return tup\n",
    "    else: \n",
    "        tup[13] = None\n",
    "        return tup\n",
    "    \n",
    "def RemoveQuoteCHdg(tup):\n",
    "    if tup[13][-1]=='\"':\n",
    "        tup[13]=tup[13][:-1]\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_traffic_clean.map(lambda tup: replaceNullValue_CHdg(RemoveQuoteCHdg(tup))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CHdg\n",
    "rdd_traffic_clean = rdd_traffic_clean.map(lambda tup: replaceNullValue_CHdg(RemoveQuoteCHdg(tup)))\n",
    "rdd_traffic_clean.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_propre=rdd_traffic_clean.filter(lambda tup:tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_propre.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress Database Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we first need to import types (e.g. StructType, StructField, IntegerType, etc.)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def getlistavion(): \n",
    "    QUERY = 'SELECT DISTINCT(TID) FROM global_temp.traffic'\n",
    "    return spark.sql(QUERY).toPandas()\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()\n",
    "\n",
    "def main_db(rdd): \n",
    "    global spark \n",
    "    # schema creation\n",
    "    \n",
    "    trafficSchema = StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                                 StructField(\"CAT\", LongType(), True),\n",
    "                                 StructField(\"TID\", StringType(), True),\n",
    "                                 StructField(\"TS\", DoubleType(), True),\n",
    "                                 StructField(\"DST\", StringType(), True),\n",
    "                                 StructField(\"SAC\", DoubleType(), True),\n",
    "                                 StructField(\"SIC\", DoubleType(), True),\n",
    "                                 StructField(\"ToD\", DoubleType(), True),\n",
    "                                 StructField(\"TN\", DoubleType(), True),\n",
    "                                 StructField(\"THETA\", DoubleType(), True),\n",
    "                                 StructField(\"RHO\", DoubleType(), True),\n",
    "                                 StructField(\"FL\", DoubleType(), True),\n",
    "                                 StructField(\"CGS\", DoubleType(), True),\n",
    "                                 StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "    \n",
    "    traffic_df_explicit = spark.createDataFrame(rdd, trafficSchema)\n",
    "    #traffic_df_explicit.show()\n",
    "    traffic_df_explicit.printSchema()\n",
    "    \n",
    "    #Creating the temporary view from the DataFrame \n",
    "    traffic_df_explicit.createOrReplaceGlobalTempView('traffic')\n",
    "    \n",
    "    #Queries\n",
    "    #print(getlistavion())\n",
    "\n",
    "main_db(rdd_traffic_clean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRadarsByPlane(tid):\n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    QUERY = 'SELECT DISTINCT(DST) FROM global_temp.traffic \\\n",
    "            WHERE TID LIKE ' + str(TID)\n",
    "    #print(QUERY)\n",
    "    return spark.sql(QUERY).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(getRadarsByPlane('TOM549'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from global_temp.traffic\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either the user choosed all the radars..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPacketsByPlane(tid):\n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    QUERY = 'SELECT * FROM global_temp.traffic \\\n",
    "            WHERE TID LIKE ' + str(TID)\n",
    "    #print(QUERY)\n",
    "    return spark.sql(QUERY).toPandas()\n",
    "\n",
    "def dictRadarsByAvion(TID):\n",
    "    df_temp = getPacketsByPlane(TID)\n",
    "    list_radar = list(df_temp.groupby('DST').size().index)\n",
    "    dictRadar = {}\n",
    "    for dst in list_radar:\n",
    "        #dictRadar[dst] = filterByPlaneAndRadar(TID, dst) ##TOO MUCH TIME\n",
    "        dictRadar[dst] = df_temp[df_temp['DST'] == dst] \n",
    "    return dictRadar\n",
    "\n",
    "import colorsys\n",
    "import random\n",
    "colorsys.hsv_to_rgb(359,100,100)\n",
    "def randomColor(i, m):\n",
    "    return rgb_to_hex(colorsys.hsv_to_rgb(float(i) / float(m), 1, 1))\n",
    "def rgb_to_hex(rgb):\n",
    "    rgbInt = (int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255))\n",
    "    return '#%02x%02x%02x' % rgbInt\n",
    "\n",
    "def plotRadar(dictRadar, var=None):\n",
    "        if var == None:\n",
    "            for var in list(dictRadar[list(dictRadar.keys())[0]].columns):\n",
    "                i, m = 0, len(dictRadar.keys())\n",
    "                for dst in dictRadar.keys():\n",
    "                    plt.plot(dictRadar[dst]['TS'], dictRadar[dst][var], randomColor(i, m))\n",
    "                    i = i + 1\n",
    "                plt.xlabel('time(s)')\n",
    "                plt.ylabel(var)\n",
    "                plt.show()            \n",
    "        else:\n",
    "            i, m = 0, len(dictRadar.keys())\n",
    "            for dst in dictRadar.keys():\n",
    "                plt.plot(dictRadar[dst]['TS'], dictRadar[dst][var], randomColor(i, m))\n",
    "                i = i + 1\n",
    "            plt.xlabel('time(s)')\n",
    "            plt.ylabel(var)\n",
    "            plt.show()  \n",
    "        \n",
    "        \n",
    "def viz(dictRadar, TID):\n",
    "    \n",
    "    plotRadar(dictRadar)\n",
    "    \n",
    "    sns.pairplot(data=getPacketsByPlane(TID), vars=['SAC', 'SIC', 'ToD', 'TN', 'THETA', 'RHO', 'FL', 'CGS', 'CHdg'])\n",
    "    plt.show()\n",
    "    \n",
    "    sns.heatmap(data=getPacketsByPlane(TID)[['SIC', 'ToD', 'TN', 'THETA', 'FL', 'CGS', 'CHdg']].corr(), annot = True, cmap = 'Reds')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "viz(dictRadarsByAvion('TOM549'), 'TOM549')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the user choosed one particular radar ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterByPlaneAndRadar(tid, dst):\n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    DST = '\\'%' + dst + '%\\'' \n",
    "    QUERY = 'SELECT * FROM global_temp.traffic WHERE TID LIKE ' + str(TID) + ' AND DST LIKE ' +  str(DST) \n",
    "    return spark.sql(QUERY).toPandas()\n",
    "\n",
    "def plotOneRadarForOnePlane(df_radar_avion):\n",
    "    for var in df_radar_avion.columns:\n",
    "        plt.plot(df_radar_avion['TS'], df_radar_avion[var])\n",
    "        plt.xlabel('time(s)')\n",
    "        plt.ylabel(var)\n",
    "        plt.show() \n",
    "        \n",
    "def viz(df_radar_avion):\n",
    "    \n",
    "    plotOneRadarForOnePlane(df_radar_avion)\n",
    "    \n",
    "    sns.pairplot(data=df_radar_avion, vars=['SAC', 'SIC', 'ToD', 'TN', 'THETA', 'RHO', 'FL', 'CGS', 'CHdg'])\n",
    "    plt.show()\n",
    "    \n",
    "    sns.heatmap(data=df_radar_avion[['SIC', 'ToD', 'TN', 'THETA', 'FL', 'CGS', 'CHdg']].corr(), annot = True, cmap = 'Reds')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "viz(filterByPlaneAndRadar('TOM549','01:00:5e:50:00:26'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries(df, var): \n",
    "\n",
    "    df_time_series_radar_avion = df.copy()\n",
    "    df_time_series_radar_avion['TS_DATE'] = df_time_series_radar_avion['TS'].copy()\n",
    "    \n",
    "    for i in range(len(df_time_series_radar_avion['TS'])):\n",
    "        df_time_series_radar_avion['TS_DATE'][i] = datetime.datetime.fromtimestamp(df_time_series_radar_avion['TS'][i].astype('int64'))\n",
    "    \n",
    "    ts = pd.Series(df_time_series_radar_avion[var].values, index = df_time_series_radar_avion['TS_DATE'])\n",
    "    ts.plot(color='r')\n",
    "    plt.title('Time Series Plot for CGS Variable over time')\n",
    "    plt.show()\n",
    "    \n",
    "    result = seasonal_decompose(ts, model='additive', period=10)\n",
    "    result.plot()\n",
    "    plt.show()\n",
    "\n",
    "    window = 20\n",
    "    ma = ts.rolling(window).mean()\n",
    "    mstd = ts.rolling(window).std()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ts.index, ts, 'r')\n",
    "    plt.plot(ma.index, ma, 'b')\n",
    "    plt.fill_between(mstd.index, ma - 2 * mstd, ma + 2 * mstd, color='b', alpha=0.2)\n",
    "    \n",
    "    ###Fbprophet###\n",
    "    \n",
    "    m = Prophet()\n",
    "    m.fit(df_time_series_radar_avion[['TS_DATE',var]].rename(columns={'TS_DATE': 'ds', var: \"y\"}))\n",
    "    future = m.make_future_dataframe(periods=20,freq='min')\n",
    "    future.tail()\n",
    "    forecast = m.predict(future)\n",
    "    m.plot(forecast)\n",
    "    #m.plot_components(forecast)\n",
    "    \n",
    "    ##ARIMA/ARMA Models \n",
    "    \n",
    "    ts_diff = ts - ts.shift()\n",
    "    diff = ts_diff.dropna()\n",
    "    plt.figure()\n",
    "    plt.plot(diff)\n",
    "    plt.title('First Difference Time Series Plot')\n",
    "    plt.show()\n",
    "    mod = ARIMA(ts, order = (1, 1, 1), seasonal_order = (2,0,0,12))\n",
    "    results = mod.fit()\n",
    "    ts.plot()\n",
    "    plt.plot(results.fittedvalues[20:], color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    predVals = results.predict(len(ts.index), len(ts.index) + 200, typ='levels')\n",
    "    dict_for = {}\n",
    "    \n",
    "    temps = df_time_series_radar_avion['TS'][len(ts.index) - 1]\n",
    "    for i in range(len(ts.index), len(ts.index) + 200):\n",
    "        dict_for[datetime.datetime.fromtimestamp(int(temps))] = predVals[i]\n",
    "        temps = temps + round(df_time_series_radar_avion['TS'][len(ts.index) - 1] - df_time_series_radar_avion['TS'][len(ts.index) - 2])\n",
    "    \n",
    "    ts.plot()\n",
    "    plt.plot(results.fittedvalues[20:], color='red')\n",
    "    plt.plot(pd.Series(dict_for), color='orange')\n",
    "    plt.show()\n",
    "    \n",
    "    window_for = 20\n",
    "    ma_for = pd.Series(dict_for).rolling(window).mean()\n",
    "    mstd_for = pd.Series(dict_for).rolling(window).std()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ts.index, ts, 'b')\n",
    "    plt.plot(results.fittedvalues[20:], color='red')\n",
    "    plt.plot(pd.Series(dict_for), color='orange')\n",
    "    plt.plot(ma_for.index, ma_for, 'b')\n",
    "    plt.fill_between(mstd_for.index, ma_for - 2 * mstd_for, ma_for + 2 * mstd_for, color='g', alpha=1)\n",
    "     \n",
    "timeseries(filterByPlaneAndRadar('TOM549','01:00:5e:50:00:26'), 'CGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT count(*) FROM global_temp.traffic\").show() \n",
    "#rdd_traffic_clean.count()\n",
    "#pyspark.sql.SparkSession.getActiveSession()\n",
    "#spark.sql(\"insert into global_temp.traffic select t.* from (select 'bidule', NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL) t\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_traffic_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with an implicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traffic_df = rdd_traffic_clean.toDF(['SRC',\n",
    "  'CAT',\n",
    "  'TID',\n",
    "  'TS',\n",
    "  'DST',\n",
    "  'SAC',\n",
    "  'SIC',\n",
    "  'ToD',\n",
    "  'TN',\n",
    "  'THETA',\n",
    "  'RHO',\n",
    "  'FL',\n",
    "  'CGS',\n",
    "  'CHdg'])\n",
    "\n",
    "traffic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traffic_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with an explicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first need to import types (e.g. StructType, StructField, IntegerType, etc.)\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema creation\n",
    "\n",
    "trafficSchema =StructType ( [StructField(\"SRC\", StringType(), True),\n",
    "                              StructField(\"CAT\", LongType(), True),\n",
    "                              StructField(\"TID\", StringType(), True),\n",
    "                              StructField(\"TS\", DoubleType(), True),\n",
    "                              StructField(\"DST\", StringType(), True),\n",
    "                              StructField(\"SAC\", DoubleType(), True),\n",
    "                              StructField(\"SIC\", DoubleType(), True),\n",
    "                              StructField(\"ToD\", DoubleType(), True),\n",
    "                              StructField(\"TN\", DoubleType(), True),\n",
    "                              StructField(\"THETA\", DoubleType(), True),\n",
    "                              StructField(\"RHO\", DoubleType(), True),\n",
    "                              StructField(\"FL\", DoubleType(), True),\n",
    "                              StructField(\"CGS\", DoubleType(), True),\n",
    "                              StructField(\"CHdg\", DoubleType(), True),\n",
    "                             ] )\n",
    "\n",
    "traffic_df_explicit = spark.createDataFrame(rdd_traffic_clean, trafficSchema)\n",
    "\n",
    "traffic_df_explicit.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL queries on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df_explicit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the temporary view from the DataFrame\n",
    "traffic_df_explicit.createOrReplaceTempView('traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM traffic\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM traffic\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT TID, COUNT(DISTINCT(DST)) FROM traffic GROUP BY TID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT TID, COUNT(*) FROM traffic group by TID ORDER BY COUNT(*) DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPacketsByPlane(tid):\n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    QUERY = 'SELECT * FROM traffic \\\n",
    "            WHERE TID LIKE ' + str(TID)\n",
    "    #print(QUERY)\n",
    "    return spark.sql(QUERY).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getPacketsByPlane('UAE222')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterByPlaneAndRadar(tid, dst):\n",
    "    TID = '\\'%' + tid + '%\\'' \n",
    "    DST = '\\'%' + dst + '%\\'' \n",
    "    QUERY = 'SELECT * FROM traffic WHERE TID LIKE ' + str(TID) + ' AND DST LIKE ' +  str(DST) \n",
    "    return spark.sql(QUERY).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterByPlaneAndRadar('DLH595' , '01:00:5e:50:00:22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictRadarsByAvion(TID):\n",
    "    df_temp = getPacketsByPlane(TID)\n",
    "    list_radar = list(df_temp.groupby('DST').size().index)\n",
    "    dictRadar = {}\n",
    "    for dst in list_radar:\n",
    "        #dictRadar[dst] = filterByPlaneAndRadar(TID, dst) ##TOO MUCH TIME\n",
    "        dictRadar[dst] = df_temp[df_temp['DST'] == dst]\n",
    "        \n",
    "    return dictRadar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictRadarsByAvion('XGO3MN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = dictRadarsByAvion('DLH595')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict['01:00:5e:50:00:22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict['01:00:5e:50:00:06']['FL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import random\n",
    "colorsys.hsv_to_rgb(359,100,100)\n",
    "def randomColor(i, m):\n",
    "    return rgb_to_hex(colorsys.hsv_to_rgb(float(i) / float(m), 1, 1))\n",
    "def rgb_to_hex(rgb):\n",
    "    rgbInt = (int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255))\n",
    "    return '#%02x%02x%02x' % rgbInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRadar(dictRadar, var=None):\n",
    "        if var == None:\n",
    "            for var in list(dictRadar[list(dictRadar.keys())[0]].columns):\n",
    "                i, m = 0, len(dictRadar.keys())\n",
    "                for dst in dictRadar.keys():\n",
    "                    plt.plot(dictRadar[dst]['TS'], dictRadar[dst][var], randomColor(i, m))\n",
    "                    i = i + 1\n",
    "                plt.xlabel('time(s)')\n",
    "                plt.ylabel(var)\n",
    "                plt.show()            \n",
    "        else:\n",
    "            i, m = 0, len(dictRadar.keys())\n",
    "            for dst in dictRadar.keys():\n",
    "                plt.plot(dictRadar[dst]['TS'], dictRadar[dst][var], randomColor(i, m))\n",
    "                i = i + 1\n",
    "            plt.xlabel('time(s)')\n",
    "            plt.ylabel(var)\n",
    "            plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotRadar(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_radar_avion = filterByPlaneAndRadar('DLH595' , '01:00:5e:50:00:22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_temp_radar_avion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_temp_radar_avion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for var in df_temp_radar_avion.columns:\n",
    "#    plt.plot(df_temp_radar_avion['TS'], df_temp_radar_avion[var])\n",
    "#    plt.xlabel('time(s)')\n",
    "#    plt.ylabel(var)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(df_temp_radar_avion['TS'], df_temp_radar_avion['SRC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOneRadarForOnePlane(df_radar_avion):\n",
    "    \n",
    "    for var in df_temp_radar_avion.columns:\n",
    "        plt.plot(df_temp_radar_avion['TS'], df_temp_radar_avion[var])\n",
    "        plt.xlabel('time(s)')\n",
    "        plt.ylabel(var)\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotOneRadarForOnePlane(df_temp_radar_avion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT count(distinct TID) FROM traffic\").show()\n",
    "#spark.sql(\"SELECT count(*) FROM traffic WHERE TID LIKE '%FRF300%'\").show()\n",
    "#spark.sql(\"SELECT count(*) FROM traffic WHERE TID = ''\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pd = spark.sql(\"SELECT TID, count(*) FROM traffic GROUP BY TID ORDER BY COUNT(*) DESC\").toPandas()\n",
    "#df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = getPacketsByPlane('DLH595')\n",
    "df_eda\n",
    "df_eda.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_eda = dictRadarsByAvion('DLH595')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotRadar(dict_eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Analysis upon all radars for one selected plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_eda, vars=['SAC', 'SIC', 'ToD', 'TN', 'THETA', 'RHO', 'FL', 'CGS', 'CHdg'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=df_eda[['SIC', 'ToD', 'TN', 'THETA', 'FL', 'CGS', 'CHdg']].corr(), annot = True, cmap = 'Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Analysis upon one selected radar for one selected plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT DST, count(*) FROM traffic \\\n",
    "#          WHERE TID LIKE '%FRF300%' GROUP BY DST ORDER BY COUNT(*) DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda_radar_avion = filterByPlaneAndRadar('DLH595' , '01:00:5e:50:00:22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_eda_radar_avion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda_radar_avion.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotOneRadarForOnePlane(df_eda_radar_avion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAT = Category du radar\n",
    "#TID = Immatriculation de l'avion\n",
    "#TS = Temps\n",
    "#DST = radar destinataire \n",
    "#SAC = System AREA Code\n",
    "#SIC = System Identification Code \n",
    "\n",
    "#ToD ???\n",
    "#TN ???\n",
    "\n",
    "#THETA : Measured position of an aircraft in local polar coordinates [0,360]\n",
    "#RH0 : Measured position of an aircraft in local polar coordinates [0,250]\n",
    "\n",
    "#CGS : Calculated track velocity expressed in polar coordinates [0,500]\n",
    "#CHDG : Calculated track velocity expressed in polar coordinates [0, 360]\n",
    "\n",
    "#FL : Flight level information [0,400]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_eda_radar_avion, vars=['SAC', 'SIC', 'ToD', 'TN', 'THETA', 'RHO', 'FL', 'CGS', 'CHdg'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(data=df_eda_radar_avion[['SIC', 'ToD', 'TN', 'THETA', 'FL', 'CGS', 'CHdg']].corr(), annot = True, cmap = 'Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certaines variables se retrouvent très corrélées, peut-on imaginer une régression pour vérifier qu'un capteur n'est pas défaillant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLLIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib is a RDD-based library.\n",
    "Another library, SparkML is developed by Spark and is based on DataFrame (see part 3).\n",
    "\n",
    "SparkML will soon completely replace MlLib library, but so far, some functionalities (especially for basic statistics) are available only on MLlib. This two libraries will coexist until spark 3 is released"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_traffic_clean.takeSample(False, 4000, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faire le diapo 5 avec ARIMA au lieu de FBprofet pour une semaine et pour les radars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "#from statsmodels.tsa.arima_model import ARIMA, ARIMAResults\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHdg ?  \n",
    "#CGS ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series_radar_avion = filterByPlaneAndRadar('DLH595' , '01:00:5e:50:00:22')\n",
    "df_time_series_radar_avion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_time_series_radar_avion['TS_DATE'] = df_time_series_radar_avion['TS'].copy()\n",
    "\n",
    "for i in range(len(df_time_series_radar_avion['TS'])):\n",
    "    df_time_series_radar_avion['TS_DATE'][i] = datetime.datetime.fromtimestamp(df_time_series_radar_avion['TS'][i].astype('int64'))\n",
    "\n",
    "df_time_series_radar_avion  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(df_time_series_radar_avion['TS_DATE'][0]).split()[1][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_time_series_radar_avion['TS_SPLIT'] = df_time_series_radar_avion['TS_DATE'].copy()\n",
    "\n",
    "for i in range(len(df_time_series_radar_avion['TS_DATE'])):\n",
    "    df_time_series_radar_avion['TS_SPLIT'][i] = str(df_time_series_radar_avion['TS_DATE'][i]).split()[1][0:5]\n",
    "    \n",
    "df_time_series_radar_avion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_time_series_radar_avion.groupby(['TS_SPLIT']).nunique()['TS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_time_series_radar_avion['TS_DATE'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CGS Variable - Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plt.plot(df_time_series_radar_avion['TS_DATE'], df_time_series_radar_avion['CGS'], color='r')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.Series(df_time_series_radar_avion['CGS'].values, index = df_time_series_radar_avion['TS_DATE'])\n",
    "\n",
    "ts.plot(color='r')\n",
    "\n",
    "#plt.plot(ts, color='r')\n",
    "plt.title('Time Series Plot for CGS Variable over time')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(ts.index[0]).split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(ts, model='additive', period=15)\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 20\n",
    "#ma == moving average\n",
    "#i compute a window on a particular set of my time series\n",
    "ma = ts.rolling(window).mean()\n",
    "mstd = ts.rolling(window).std()\n",
    "#std == standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = ts.rolling(window).mean()\n",
    "ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ts.index[0:300], ts[0:300], 'r')\n",
    "plt.plot(ma.index[0:300], ma[0:300], 'b')\n",
    "plt.fill_between(mstd.index[0:300], ma[0:300] - 2 * mstd[0:300], ma[0:300] + 2 * mstd[0:300], color='b', alpha=0.2)\n",
    "#Hoddric prescot filter\n",
    "#Yt= f(Yt,lambda) + Noise\n",
    "#Yt=[Yt[]N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peut-être qu'avec la moving_average trend, je peux avoir une prédiction ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profet - fbplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_time_series_radar_avion[['TS_DATE','CGS']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.rename(columns={'TS_DATE': 'ds', 'CGS': \"y\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = Prophet()\n",
    "m.fit(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=20,freq='min')\n",
    "future.tail()\n",
    "forecast = m.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.plot(forecast)\n",
    "m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast.to_csv(r'forecast.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA/ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIAGNOSING ACF\n",
    "acf = plot_acf(ts, lags = 40)\n",
    "plt.title(\"ACF Plot\")\n",
    "acf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DIAGNOSING PACF\n",
    "pacf = plot_pacf(ts, lags = 20)\n",
    "plt.title(\"PACF Plot\")\n",
    "pacf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KPSS test\n",
    "def kpss_test(series, **kw):    \n",
    "    statistic, p_value, n_lags, critical_values = kpss(series, **kw)\n",
    "    # Format Output\n",
    "    print(f'KPSS Statistic: {statistic}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    print(f'num lags: {n_lags}')\n",
    "    print('Critial Values:')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'   {key} : {value}')\n",
    "    print(f'Result: The series is {\"not \" if p_value < 0.05 else \"\"}stationary')\n",
    "\n",
    "kpss_test(ts)\n",
    "#print('\\n')\n",
    "#kpss_test(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMING OUR DATA TO ADJUST FOR NON-STATIONARITY\n",
    "ts_diff = ts - ts.shift()\n",
    "diff = ts_diff.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kpss_test(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have found a serie which is stationary, we can move to the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(diff)\n",
    "plt.title('First Difference Time Series Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acfDiff = plot_acf(diff, lags = 80)\n",
    "plt.title(\"ACF Plot\")\n",
    "acfDiff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pacfDiff = plot_pacf(diff, lags = 80)\n",
    "plt.title(\"PACF Plot\")\n",
    "pacfDiff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box-Jenkins methodology to buid an ARIMA process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.auto_arima(ts, seasonal=True, m=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#mod = ARIMA(ts, order = (0, 1, 2))\n",
    "\n",
    "#mod = ARIMA(ts, order = (3, 1, 1), seasonal_order = (0,1,0,12))\n",
    "\n",
    "mod = ARIMA(ts, order = (1, 1, 1), seasonal_order = (2,0,0,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts.plot()\n",
    "plt.plot(results.fittedvalues[1:], color='red')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.fittedvalues[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index\n",
    "predVals = results.predict(383, 600, typ='levels')\n",
    "print(predVals)\n",
    "#type(predVals)\n",
    "dict_for = {}\n",
    "temps = df_time_series_radar_avion['TS'][378]\n",
    "for i in range(383, 601):\n",
    "    dict_for[datetime.datetime.fromtimestamp(int(temps))] = predVals[i]\n",
    "    temps = temps + 5\n",
    "ts.plot()\n",
    "plt.plot(results.fittedvalues[13:], color='red')\n",
    "plt.plot(pd.Series(dict_for), color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predVals = results.predict(383, 600, typ='levels')\n",
    "print(predVals)\n",
    "#type(predVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predVals.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series_radar_avion['TS'][378]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = df_time_series_radar_avion['TS'][382] + 5\n",
    "\n",
    "datetime.datetime.fromtimestamp(int(temps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for = {}\n",
    "temps = df_time_series_radar_avion['TS'][378]\n",
    "for i in range(383, 601):\n",
    "    dict_for[datetime.datetime.fromtimestamp(int(temps))] = predVals[i]\n",
    "    temps = temps + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(dict_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#result_temp = pd.Series(dict_for)\n",
    "#result_temp.index.set_names('TS_DATE', inplace=True)\n",
    "pd.Series(dict_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.plot()\n",
    "plt.plot(results.fittedvalues[13:], color='red')\n",
    "plt.plot(pd.Series(dict_for), color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_for = 20\n",
    "ma_for = pd.Series(dict_for).rolling(window).mean()\n",
    "mstd_for = pd.Series(dict_for).rolling(window).std()\n",
    "#std == standard deviation\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ts.index, ts, 'b')\n",
    "plt.plot(results.fittedvalues[13:], color='red')\n",
    "plt.plot(pd.Series(dict_for), color='orange')\n",
    "plt.plot(ma_for.index, ma_for, 'b')\n",
    "plt.fill_between(mstd_for.index, ma_for - 2 * mstd_for, ma_for + 2 * mstd_for, color='g', alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMAX - Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(ts, order=(0,1,2), enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
